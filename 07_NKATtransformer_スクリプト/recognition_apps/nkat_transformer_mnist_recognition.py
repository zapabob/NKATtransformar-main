#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT-Transformerçµ±åˆMNISTç”»åƒèªè­˜ã‚·ã‚¹ãƒ†ãƒ 
RTX3080æœ€é©åŒ–ãƒ»é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œãƒ»é•·æ™‚é–“å­¦ç¿’

NKAT-Vision Transformerçµ±åˆç†è«–ï¼š
- ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ã‚’æŒã¤ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
- éå¯æ›å¹¾ä½•å­¦çš„æ³¨æ„æ©Ÿæ§‹
- é‡å­é‡åŠ›è£œæ­£é …ã«ã‚ˆã‚‹ç”»åƒç‰¹å¾´æŠ½å‡º
- è¶…åæŸå› å­ã«ã‚ˆã‚‹é«˜ç²¾åº¦èªè­˜

Author: NKAT Advanced Computing Team
Date: 2025-01-26
CUDA Requirement: RTX3080 or higher
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import json
import pickle
import os
import time
import math
from datetime import datetime
from tqdm import tqdm
import logging
import warnings
warnings.filterwarnings('ignore')

# CUDAæœ€é©åŒ–
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NKATVisionConfig:
    """NKAT-Vision Transformerè¨­å®šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # ç”»åƒè¨­å®š
        self.image_size = 28            # MNISTç”»åƒã‚µã‚¤ã‚º
        self.patch_size = 7             # ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º (4x4ãƒ‘ãƒƒãƒ)
        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.channels = 1               # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.d_model = 384              # ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
        self.nhead = 6                  # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ³¨æ„æ©Ÿæ§‹ã®ãƒ˜ãƒƒãƒ‰æ•°
        self.num_layers = 8             # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        self.dim_feedforward = 1536     # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æ¬¡å…ƒ
        self.dropout = 0.1              # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        self.num_classes = 10           # MNISTåˆ†é¡æ•°
        
        # NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta_nc = 1e-35           # éå¯æ›æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma_conv = 2.718         # è¶…åæŸå› å­
        self.quantum_correction = 1e-6  # é‡å­é‡åŠ›è£œæ­£å¼·åº¦
        self.gauge_symmetry_dim = 8     # ã‚²ãƒ¼ã‚¸å¯¾ç§°æ€§æ¬¡å…ƒ
        
        # è¨“ç·´è¨­å®š
        self.batch_size = 128           # RTX3080æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º
        self.num_epochs = 200           # ã‚¨ãƒãƒƒã‚¯æ•°
        self.learning_rate = 1e-3       # å­¦ç¿’ç‡
        self.warmup_steps = 1000        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—
        self.weight_decay = 1e-4        # é‡ã¿æ¸›è¡°
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µè¨­å®š
        self.use_augmentation = True    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µä½¿ç”¨
        self.rotation_degrees = 10      # å›è»¢è§’åº¦
        self.zoom_factor = 0.1          # ã‚ºãƒ¼ãƒ ä¿‚æ•°
        
        # ãƒªã‚«ãƒãƒªãƒ¼è¨­å®š
        self.checkpoint_interval = 10   # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”
        self.auto_save_interval = 300   # è‡ªå‹•ä¿å­˜é–“éš”ï¼ˆç§’ï¼‰

class NKATPatchEmbedding(nn.Module):
    """NKATç†è«–ã«åŸºã¥ããƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ãƒ‘ãƒƒãƒæŠ•å½±
        self.patch_projection = nn.Conv2d(
            config.channels, 
            config.d_model, 
            kernel_size=config.patch_size, 
            stride=config.patch_size
        )
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        self.position_embedding = nn.Parameter(
            torch.randn(1, config.num_patches + 1, config.d_model) * 0.02
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.cls_token = nn.Parameter(
            torch.randn(1, 1, config.d_model) * 0.02
        )
        
        # éå¯æ›å¹¾ä½•å­¦çš„è£œæ­£
        self.nc_correction = nn.Parameter(
            torch.randn(config.d_model) * config.theta_nc
        )
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gauge_params = nn.Parameter(
            torch.randn(config.gauge_symmetry_dim, config.d_model) * 0.01
        )
        
        # é‡å­é‡åŠ›è£œæ­£å±¤
        self.quantum_layer = nn.Sequential(
            nn.Linear(config.d_model, config.d_model),
            nn.GELU(),
            nn.Linear(config.d_model, config.d_model)
        )
        
        self.dropout = nn.Dropout(config.dropout)
        
    def forward(self, x):
        batch_size = x.shape[0]
        
        # ãƒ‘ãƒƒãƒæŠ•å½± (batch_size, channels, height, width) -> (batch_size, d_model, n_patches_h, n_patches_w)
        x = self.patch_projection(x)  # (B, d_model, 4, 4)
        
        # ãƒ•ãƒ©ãƒƒãƒˆåŒ– (batch_size, d_model, num_patches)
        x = x.flatten(2).transpose(1, 2)  # (B, 16, d_model)
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®è¿½åŠ 
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, 17, d_model)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿ã®è¿½åŠ 
        x = x + self.position_embedding
        
        # éå¯æ›å¹¾ä½•å­¦çš„è£œæ­£ã®é©ç”¨
        nc_effect = self.nc_correction.unsqueeze(0).unsqueeze(0)
        x = x + nc_effect
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ã®é©ç”¨
        gauge_effect = torch.einsum('gd,bd->bg', self.gauge_params, x.mean(dim=1))
        gauge_correction = torch.einsum('bg,gd->bd', gauge_effect, self.gauge_params)
        x = x + gauge_correction.unsqueeze(1) * self.config.quantum_correction
        
        # é‡å­é‡åŠ›è£œæ­£
        quantum_correction = self.quantum_layer(x) * self.config.quantum_correction
        x = x + quantum_correction
        
        return self.dropout(x)

class NKATGaugeInvariantAttention(nn.Module):
    """NKATç†è«–ã«åŸºã¥ãã‚²ãƒ¼ã‚¸ä¸å¤‰æ³¨æ„æ©Ÿæ§‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.d_model = config.d_model
        self.nhead = config.nhead
        self.d_k = self.d_model // self.nhead
        
        # ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼æŠ•å½±
        self.w_q = nn.Linear(self.d_model, self.d_model, bias=False)
        self.w_k = nn.Linear(self.d_model, self.d_model, bias=False)
        self.w_v = nn.Linear(self.d_model, self.d_model, bias=False)
        self.w_o = nn.Linear(self.d_model, self.d_model)
        
        # ã‚²ãƒ¼ã‚¸å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gauge_phase = nn.Parameter(torch.zeros(self.nhead))
        self.gauge_amplitude = nn.Parameter(torch.ones(self.nhead))
        
        # éå¯æ›è£œæ­£é …
        self.nc_mixing = nn.Parameter(torch.randn(self.nhead, self.d_k, self.d_k) * config.theta_nc)
        
        self.dropout = nn.Dropout(config.dropout)
        self.scale = math.sqrt(self.d_k)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.size()
        
        # QKVè¨ˆç®—
        Q = self.w_q(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        K = self.w_k(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        V = self.w_v(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ã®é©ç”¨
        gauge_rotation = torch.exp(1j * self.gauge_phase.view(1, -1, 1, 1))
        gauge_scale = self.gauge_amplitude.view(1, -1, 1, 1)
        
        # è¤‡ç´ ã‚²ãƒ¼ã‚¸å¤‰æ›ï¼ˆå®Ÿéƒ¨ã®ã¿ä½¿ç”¨ï¼‰
        Q_gauge = Q * gauge_scale * gauge_rotation.real
        K_gauge = K * gauge_scale * gauge_rotation.real
        
        # éå¯æ›è£œæ­£ã®é©ç”¨
        Q_nc = torch.einsum('bhnd,hdk->bhnk', Q_gauge, self.nc_mixing)
        K_nc = torch.einsum('bhnd,hdk->bhnk', K_gauge, self.nc_mixing)
        
        # ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿å†…ç©æ³¨æ„
        scores = torch.matmul(Q_nc, K_nc.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # æ³¨æ„æ©Ÿæ§‹ã®é©ç”¨
        context = torch.matmul(attn_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        return self.w_o(context), attn_weights

class NKATSuperConvergenceBlock(nn.Module):
    """NKATè¶…åæŸãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.d_model = config.d_model
        
        # è¶…åæŸå¤‰æ›
        self.convergence_transform = nn.Sequential(
            nn.Linear(self.d_model, self.d_model * 4),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(self.d_model * 4, self.d_model),
            nn.Dropout(config.dropout)
        )
        
        # åæŸåˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.alpha = nn.Parameter(torch.tensor(config.gamma_conv))
        self.beta = nn.Parameter(torch.tensor(1.0))
        self.gamma = nn.Parameter(torch.tensor(0.1))
        
        # éç·šå½¢åæŸé–¢æ•°
        self.convergence_activation = nn.Sequential(
            nn.Linear(self.d_model, self.d_model),
            nn.Tanh(),
            nn.Linear(self.d_model, self.d_model)
        )
        
    def forward(self, x):
        # è¶…åæŸå¤‰æ›
        transformed = self.convergence_transform(x)
        
        # åæŸåˆ¶å¾¡ã®è¨ˆç®—
        x_norm = torch.norm(x, dim=-1, keepdim=True)
        convergence_weight = torch.exp(-self.alpha * x_norm)
        
        # éç·šå½¢åæŸé …
        nonlinear_term = self.convergence_activation(x) * self.gamma
        
        # æœ€çµ‚çš„ãªè¶…åæŸå‡ºåŠ›
        output = (self.beta * x + 
                 convergence_weight * transformed + 
                 nonlinear_term)
        
        return output

class NKATTransformerBlock(nn.Module):
    """NKAT-Transformerãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ³¨æ„æ©Ÿæ§‹
        self.attention = NKATGaugeInvariantAttention(config)
        
        # è¶…åæŸãƒ–ãƒ­ãƒƒã‚¯
        self.super_convergence = NKATSuperConvergenceBlock(config)
        
        # å±¤æ­£è¦åŒ–
        self.norm1 = nn.LayerNorm(config.d_model)
        self.norm2 = nn.LayerNorm(config.d_model)
        self.norm3 = nn.LayerNorm(config.d_model)
        
        # æ®‹å·®æ¥ç¶šã®é‡ã¿
        self.residual_weight = nn.Parameter(torch.tensor(1.0))
        
    def forward(self, x, mask=None):
        # Pre-normæ³¨æ„æ©Ÿæ§‹
        norm_x = self.norm1(x)
        attn_output, attn_weights = self.attention(norm_x, mask)
        x = x + self.residual_weight * attn_output
        
        # Pre-normè¶…åæŸ
        norm_x = self.norm2(x)
        conv_output = self.super_convergence(norm_x)
        x = x + self.residual_weight * conv_output
        
        # æœ€çµ‚æ­£è¦åŒ–
        x = self.norm3(x)
        
        return x, attn_weights

class NKATVisionTransformer(nn.Module):
    """NKAT-Vision Transformerãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        self.patch_embedding = NKATPatchEmbedding(config)
        
        # Transformerãƒ–ãƒ­ãƒƒã‚¯
        self.transformer_blocks = nn.ModuleList([
            NKATTransformerBlock(config) for _ in range(config.num_layers)
        ])
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(config.d_model),
            nn.Linear(config.d_model, config.d_model // 2),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_model // 2, config.num_classes)
        )
        
        # é‡å­é‡åŠ›è£œæ­£é …
        self.quantum_classifier = nn.Sequential(
            nn.Linear(config.d_model, config.d_model),
            nn.Tanh(),
            nn.Linear(config.d_model, config.num_classes)
        )
        
        # çµ±åˆé‡ã¿
        self.fusion_weight = nn.Parameter(torch.tensor(0.1))
        
    def forward(self, x):
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        x = self.patch_embedding(x)  # (B, num_patches+1, d_model)
        
        # Transformerãƒ–ãƒ­ãƒƒã‚¯ã®é †æ¬¡é©ç”¨
        attention_weights = []
        for block in self.transformer_blocks:
            x, attn_weights = block(x)
            attention_weights.append(attn_weights)
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®æŠ½å‡º
        cls_token = x[:, 0]  # (B, d_model)
        
        # åˆ†é¡äºˆæ¸¬
        main_logits = self.classifier(cls_token)
        quantum_logits = self.quantum_classifier(cls_token)
        
        # é‡å­é‡åŠ›çµ±åˆ
        final_logits = main_logits + self.fusion_weight * quantum_logits
        
        return {
            'logits': final_logits,
            'cls_features': cls_token,
            'attention_weights': attention_weights,
            'quantum_contribution': quantum_logits
        }

class NKATMNISTTrainer:
    """NKAT-MNISTè¨“ç·´ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.model = NKATVisionTransformer(config).to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=50,
            T_mult=2,
            eta_min=1e-6
        )
        
        # æå¤±é–¢æ•°
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # è¨“ç·´è¨˜éŒ²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†
        self.checkpoint_dir = "nkat_mnist_checkpoints"
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.best_val_acc = 0.0
        self.last_save_time = time.time()
        
    def save_checkpoint(self, epoch, val_acc, extra_info=None):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        is_best = val_acc > self.best_val_acc
        if is_best:
            self.best_val_acc = val_acc
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_history': self.train_history,
            'best_val_acc': self.best_val_acc,
            'config': self.config.__dict__,
            'timestamp': datetime.now().isoformat()
        }
        
        if extra_info:
            checkpoint.update(extra_info)
        
        # é€šå¸¸ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        checkpoint_path = os.path.join(
            self.checkpoint_dir,
            f"checkpoint_epoch_{epoch:04d}_acc_{val_acc:.4f}.pt"
        )
        torch.save(checkpoint, checkpoint_path)
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, "best_model.pt")
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ† æ–°è¨˜éŒ²! ç²¾åº¦: {val_acc:.4f}")
        
        # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        latest_path = os.path.join(self.checkpoint_dir, "latest_checkpoint.pt")
        torch.save(checkpoint, latest_path)
        
        logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: ã‚¨ãƒãƒƒã‚¯ {epoch}, ç²¾åº¦ {val_acc:.4f}")
    
    def load_checkpoint(self, checkpoint_path=None):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        if checkpoint_path is None:
            checkpoint_path = os.path.join(self.checkpoint_dir, "latest_checkpoint.pt")
        
        if not os.path.exists(checkpoint_path):
            logger.info("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            return 0
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.train_history = checkpoint['train_history']
            self.best_val_acc = checkpoint.get('best_val_acc', 0.0)
            
            start_epoch = checkpoint['epoch'] + 1
            logger.info(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©å…ƒ: ã‚¨ãƒãƒƒã‚¯ {start_epoch}, ãƒ™ã‚¹ãƒˆç²¾åº¦ {self.best_val_acc:.4f}")
            return start_epoch
            
        except Exception as e:
            logger.error(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def train_epoch(self, train_loader, epoch):
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(train_loader, desc=f'Train Epoch {epoch:03d}')
        
        for batch_idx, (data, target) in enumerate(progress_bar):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            
            # é †ä¼æ’­
            outputs = self.model(data)
            loss = self.criterion(outputs['logits'], target)
            
            # é€†ä¼æ’­
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # çµ±è¨ˆæ›´æ–°
            total_loss += loss.item()
            pred = outputs['logits'].argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            if batch_idx % 10 == 0:
                progress_bar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Acc': f"{100.*correct/total:.2f}%",
                    'GPU': f"{torch.cuda.memory_allocated()/1e9:.1f}GB"
                })
            
            # è‡ªå‹•ä¿å­˜ãƒã‚§ãƒƒã‚¯
            if time.time() - self.last_save_time > self.config.auto_save_interval:
                # ç°¡æ˜“æ¤œè¨¼
                val_acc = self.quick_validate()
                self.save_checkpoint(epoch, val_acc, {'batch_idx': batch_idx})
                self.last_save_time = time.time()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate_epoch(self, val_loader, epoch):
        """æ¤œè¨¼ã‚¨ãƒãƒƒã‚¯"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            progress_bar = tqdm(val_loader, desc=f'Val Epoch {epoch:03d}')
            
            for data, target in progress_bar:
                data, target = data.to(self.device), target.to(self.device)
                
                outputs = self.model(data)
                loss = self.criterion(outputs['logits'], target)
                
                total_loss += loss.item()
                pred = outputs['logits'].argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
                
                progress_bar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Acc': f"{100.*correct/total:.2f}%"
                })
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def quick_validate(self):
        """ç°¡æ˜“æ¤œè¨¼ï¼ˆè‡ªå‹•ä¿å­˜ç”¨ï¼‰"""
        self.model.eval()
        # å°‘æ•°ã®ãƒãƒƒãƒã§ç°¡æ˜“æ¤œè¨¼
        return self.best_val_acc  # ç°¡ç•¥åŒ–
    
    def train(self, train_loader, val_loader, start_epoch=0):
        """ãƒ¡ã‚¤ãƒ³è¨“ç·´ãƒ«ãƒ¼ãƒ—"""
        logger.info("ğŸš€ NKAT-MNISTè¨“ç·´é–‹å§‹")
        
        try:
            for epoch in range(start_epoch, self.config.num_epochs):
                # è¨“ç·´
                train_loss, train_acc = self.train_epoch(train_loader, epoch)
                
                # æ¤œè¨¼
                val_loss, val_acc = self.validate_epoch(val_loader, epoch)
                
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°
                self.scheduler.step()
                
                # å±¥æ­´è¨˜éŒ²
                self.train_history['epoch'].append(epoch)
                self.train_history['train_loss'].append(train_loss)
                self.train_history['train_acc'].append(train_acc)
                self.train_history['val_loss'].append(val_loss)
                self.train_history['val_acc'].append(val_acc)
                
                # ãƒ­ã‚°å‡ºåŠ›
                logger.info(
                    f"Epoch {epoch:03d} - "
                    f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - "
                    f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%"
                )
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if (epoch + 1) % self.config.checkpoint_interval == 0:
                    self.save_checkpoint(epoch, val_acc)
                
                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ è¨“ç·´ä¸­æ–­ - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­...")
            self.save_checkpoint(epoch, val_acc, {'interrupted': True})
        
        except Exception as e:
            logger.error(f"ğŸ’¥ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            self.save_checkpoint(epoch, val_acc, {'error': str(e)})
            raise
        
        finally:
            self.save_checkpoint(epoch, val_acc, {'training_completed': True})
            logger.info("âœ… è¨“ç·´å®Œäº†")

def create_data_loaders(config):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
    
    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®å®šç¾©
    if config.use_augmentation:
        train_transform = transforms.Compose([
            transforms.RandomRotation(config.rotation_degrees),
            transforms.RandomAffine(0, scale=(1-config.zoom_factor, 1+config.zoom_factor)),
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
    else:
        train_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
    
    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    train_dataset = torchvision.datasets.MNIST(
        root='./data', 
        train=True,
        download=True, 
        transform=train_transform
    )
    
    val_dataset = torchvision.datasets.MNIST(
        root='./data', 
        train=False,
        download=True, 
        transform=val_transform
    )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,  # Windowsäº’æ›æ€§
        pin_memory=torch.cuda.is_available()
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=torch.cuda.is_available()
    )
    
    logger.info(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)}, æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_dataset)}")
    
    return train_loader, val_loader

def visualize_results(trainer, config):
    """çµæœå¯è¦–åŒ–"""
    history = trainer.train_history
    
    if len(history['epoch']) == 0:
        logger.warning("è¨“ç·´å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = history['epoch']
    
    # æå¤±æ›²ç·š
    ax1.plot(epochs, history['train_loss'], label='Train Loss', color='blue')
    ax1.plot(epochs, history['val_loss'], label='Val Loss', color='red')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('NKAT-Transformer MNIST Loss Curves')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç²¾åº¦æ›²ç·š
    ax2.plot(epochs, history['train_acc'], label='Train Accuracy', color='blue')
    ax2.plot(epochs, history['val_acc'], label='Val Accuracy', color='red')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('NKAT-Transformer MNIST Accuracy Curves')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å­¦ç¿’ç‡æ›²ç·š
    lrs = [trainer.optimizer.param_groups[0]['lr']] * len(epochs)
    ax3.plot(epochs, lrs, color='green')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Learning Rate')
    ax3.set_title('Learning Rate Schedule')
    ax3.grid(True, alpha=0.3)
    
    # æœ€çµ‚æ€§èƒ½ã‚µãƒãƒªãƒ¼
    best_val_acc = max(history['val_acc']) if history['val_acc'] else 0
    final_train_acc = history['train_acc'][-1] if history['train_acc'] else 0
    
    ax4.text(0.1, 0.7, f'Best Validation Accuracy: {best_val_acc:.2f}%', 
             fontsize=14, fontweight='bold')
    ax4.text(0.1, 0.5, f'Final Training Accuracy: {final_train_acc:.2f}%', 
             fontsize=14)
    ax4.text(0.1, 0.3, f'Total Epochs: {len(epochs)}', fontsize=14)
    ax4.text(0.1, 0.1, f'Model Parameters: {sum(p.numel() for p in trainer.model.parameters()):,}', 
             fontsize=12)
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    ax4.set_title('Training Summary')
    
    plt.tight_layout()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plt.savefig(f'nkat_mnist_training_results_{timestamp}.png', 
               dpi=300, bbox_inches='tight')
    logger.info("ğŸ“Š è¨“ç·´çµæœå¯è¦–åŒ–å®Œäº†")
    
    return fig

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ NKAT-Transformer MNISTç”»åƒèªè­˜ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    # è¨­å®šåˆæœŸåŒ–
    config = NKATVisionConfig()
    logger.info(f"ğŸ–¥ï¸ CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"ğŸ® GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™
    logger.info("ğŸ“Š MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­...")
    train_loader, val_loader = create_data_loaders(config)
    
    # è¨“ç·´ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    trainer = NKATMNISTTrainer(config)
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
    total_params = sum(p.numel() for p in trainer.model.parameters())
    logger.info(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
    
    # ãƒªã‚«ãƒãƒªãƒ¼ãƒã‚§ãƒƒã‚¯
    start_epoch = trainer.load_checkpoint()
    
    # è¨“ç·´å®Ÿè¡Œ
    trainer.train(train_loader, val_loader, start_epoch)
    
    # çµæœå¯è¦–åŒ–
    visualize_results(trainer, config)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = {
        'timestamp': datetime.now().isoformat(),
        'config': config.__dict__,
        'model_info': {
            'total_parameters': total_params,
            'architecture': 'NKAT-Vision Transformer',
            'patch_size': config.patch_size,
            'num_patches': config.num_patches
        },
        'training_results': {
            'best_val_accuracy': trainer.best_val_acc,
            'total_epochs': len(trainer.train_history['epoch']),
            'final_train_acc': trainer.train_history['train_acc'][-1] if trainer.train_history['train_acc'] else 0
        },
        'nkat_features': {
            'gauge_invariance': True,
            'non_commutative_geometry': True,
            'super_convergence': True,
            'quantum_gravity_correction': True
        },
        'gpu_info': {
            'device': str(trainer.device),
            'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0
        }
    }
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f'nkat_mnist_report_{timestamp}.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info("ğŸ¯ NKAT-Transformer MNISTèªè­˜å®Œäº†!")
    logger.info(f"ğŸ† æœ€é«˜æ¤œè¨¼ç²¾åº¦: {trainer.best_val_acc:.2f}%")
    
    return report

if __name__ == "__main__":
    try:
        report = main()
        print(f"\nâœ… è¨ˆç®—å®Œäº†!")
        print(f"ğŸ“ çµæœãƒ¬ãƒãƒ¼ãƒˆ: nkat_mnist_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        print(f"ğŸ† æœ€é«˜ç²¾åº¦: {report['training_results']['best_val_accuracy']:.2f}%")
    except Exception as e:
        logger.error(f"ğŸ’¥ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        raise 