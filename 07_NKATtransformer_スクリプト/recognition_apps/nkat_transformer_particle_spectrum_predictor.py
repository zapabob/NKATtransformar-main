#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT-Transformerçµ±åˆç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
RTX3080æœ€é©åŒ–ãƒ»é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œãƒ»é•·æ™‚é–“è¨ˆç®—

Transformerãƒ¢ãƒ‡ãƒ«ã¨ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ï¼š
- æœ€æ–°ç ”ç©¶ã«ã‚ˆã‚ŠTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ã‚’æŒã¤ã“ã¨ãŒåˆ¤æ˜
- NKATç†è«–ã®éå¯æ›ã‚²ãƒ¼ã‚¸å¯¾ç§°æ€§ã¨è‡ªç„¶ã«æ•´åˆ
- ãƒŸãƒ¥ãƒ¼ã‚ªãƒ³g-2ç•°å¸¸ã«åŸºã¥ãç¬¬äº”ã®åŠ›ã‚’å‰æã¨ã—ãŸäºˆæ¸¬

Author: NKAT Advanced Computing Team
Date: 2025-01-26
CUDA Requirement: RTX3080 or higher
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import json
import pickle
import os
import time
from datetime import datetime
from tqdm import tqdm
import logging
import warnings
warnings.filterwarnings('ignore')

# CUDAæœ€é©åŒ–
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NKATTransformerConfig:
    """NKAT-Transformerè¨­å®šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.d_model = 512          # ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
        self.nhead = 8              # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ³¨æ„æ©Ÿæ§‹ã®ãƒ˜ãƒƒãƒ‰æ•°
        self.num_layers = 12        # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        self.dim_feedforward = 2048 # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æ¬¡å…ƒ
        self.dropout = 0.1          # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        self.max_seq_len = 256      # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        
        # ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.particle_types = 6     # NKATäºˆæ¸¬ç²’å­æ•°
        self.mass_range = 54        # è³ªé‡ç¯„å›²ï¼ˆæ¡æ•°ï¼‰
        self.energy_levels = 128    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ¬ãƒ™ãƒ«æ•°
        
        # NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta_nc = 1e-35       # éå¯æ›æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma_conv = 2.718     # è¶…åæŸå› å­ãƒ™ãƒ¼ã‚¹
        self.fifth_force_strength = 251e-11  # ãƒŸãƒ¥ãƒ¼ã‚ªãƒ³g-2ã‹ã‚‰å°å‡º
        
        # è¨ˆç®—è¨­å®š
        self.batch_size = 16        # RTX3080æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º
        self.num_epochs = 1000      # ã‚¨ãƒãƒƒã‚¯æ•°
        self.learning_rate = 1e-4   # å­¦ç¿’ç‡
        self.warmup_steps = 1000    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—
        
        # ãƒªã‚«ãƒãƒªãƒ¼è¨­å®š
        self.checkpoint_interval = 100  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”
        self.auto_save_interval = 300   # è‡ªå‹•ä¿å­˜é–“éš”ï¼ˆç§’ï¼‰

class GaugeInvariantAttention(nn.Module):
    """ã‚²ãƒ¼ã‚¸ä¸å¤‰ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ³¨æ„æ©Ÿæ§‹"""
    
    def __init__(self, d_model, nhead, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ã‚’ä¿æŒã™ã‚‹ç·šå½¢å¤‰æ›
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        
        # ã‚²ãƒ¼ã‚¸å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gauge_phase = nn.Parameter(torch.zeros(nhead))
        
        self.dropout = nn.Dropout(dropout)
        self.scale = np.sqrt(self.d_k)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.size()
        
        # ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã®è¨ˆç®—
        Q = self.w_q(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        K = self.w_k(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        V = self.w_v(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ€§ã®é©ç”¨
        gauge_factor = torch.exp(1j * self.gauge_phase.view(1, -1, 1, 1))
        Q = Q * gauge_factor.real - V * gauge_factor.imag
        
        # ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿å†…ç©æ³¨æ„æ©Ÿæ§‹
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = torch.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # æ³¨æ„æ©Ÿæ§‹ã®é©ç”¨
        context = torch.matmul(attn_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model)
        
        return self.w_o(context), attn_weights

class NKATEmbedding(nn.Module):
    """NKATç†è«–ã«åŸºã¥ãç‰©ç†åŸ‹ã‚è¾¼ã¿"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.d_model = config.d_model
        
        # ç‰©ç†é‡åŸ‹ã‚è¾¼ã¿
        self.mass_embedding = nn.Linear(1, config.d_model // 4)
        self.energy_embedding = nn.Linear(1, config.d_model // 4)
        self.coupling_embedding = nn.Linear(1, config.d_model // 4)
        self.gauge_embedding = nn.Linear(1, config.d_model // 4)
        
        # éå¯æ›è£œæ­£é …
        self.nc_correction = nn.Parameter(torch.randn(config.d_model) * config.theta_nc)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        self.pos_embedding = nn.Parameter(torch.randn(config.max_seq_len, config.d_model))
        
    def forward(self, mass, energy, coupling, gauge_param):
        batch_size = mass.size(0)
        seq_len = mass.size(1)
        
        # å„ç‰©ç†é‡ã®åŸ‹ã‚è¾¼ã¿
        mass_emb = self.mass_embedding(mass.unsqueeze(-1))
        energy_emb = self.energy_embedding(energy.unsqueeze(-1))
        coupling_emb = self.coupling_embedding(coupling.unsqueeze(-1))
        gauge_emb = self.gauge_embedding(gauge_param.unsqueeze(-1))
        
        # çµ±åˆåŸ‹ã‚è¾¼ã¿
        embedding = torch.cat([mass_emb, energy_emb, coupling_emb, gauge_emb], dim=-1)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿ã®è¿½åŠ 
        embedding = embedding + self.pos_embedding[:seq_len, :].unsqueeze(0)
        
        # éå¯æ›è£œæ­£ã®é©ç”¨
        embedding = embedding + self.nc_correction.unsqueeze(0).unsqueeze(0)
        
        return embedding

class SuperConvergenceBlock(nn.Module):
    """è¶…åæŸå› å­ãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, d_model, convergence_factor=2.718):
        super().__init__()
        self.d_model = d_model
        self.convergence_factor = convergence_factor
        
        # è¶…åæŸå¤‰æ›
        self.conv_transform = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Linear(d_model * 2, d_model),
            nn.Dropout(0.1)
        )
        
        # åæŸåˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.alpha = nn.Parameter(torch.tensor(convergence_factor))
        self.beta = nn.Parameter(torch.tensor(1.0))
        
    def forward(self, x):
        # è¶…åæŸå¤‰æ›
        transformed = self.conv_transform(x)
        
        # åæŸå› å­ã®é©ç”¨
        convergence_weight = torch.exp(-self.alpha * torch.norm(x, dim=-1, keepdim=True))
        
        # åæŸåˆ¶å¾¡
        output = self.beta * x + convergence_weight * transformed
        
        return output

class NKATTransformerLayer(nn.Module):
    """NKAT Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ã‚²ãƒ¼ã‚¸ä¸å¤‰æ³¨æ„æ©Ÿæ§‹
        self.attention = GaugeInvariantAttention(
            config.d_model, config.nhead, config.dropout)
        
        # è¶…åæŸãƒ–ãƒ­ãƒƒã‚¯
        self.super_convergence = SuperConvergenceBlock(
            config.d_model, config.gamma_conv)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.feed_forward = nn.Sequential(
            nn.Linear(config.d_model, config.dim_feedforward),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.dim_feedforward, config.d_model),
            nn.Dropout(config.dropout)
        )
        
        # å±¤æ­£è¦åŒ–
        self.norm1 = nn.LayerNorm(config.d_model)
        self.norm2 = nn.LayerNorm(config.d_model)
        self.norm3 = nn.LayerNorm(config.d_model)
        
    def forward(self, x, mask=None):
        # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ³¨æ„æ©Ÿæ§‹
        attn_output, attn_weights = self.attention(self.norm1(x), mask)
        x = x + attn_output
        
        # è¶…åæŸãƒ–ãƒ­ãƒƒã‚¯
        conv_output = self.super_convergence(self.norm2(x))
        x = x + conv_output
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        ff_output = self.feed_forward(self.norm3(x))
        x = x + ff_output
        
        return x, attn_weights

class NKATTransformerModel(nn.Module):
    """NKAT-Transformerçµ±åˆãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ç‰©ç†åŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = NKATEmbedding(config)
        
        # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.layers = nn.ModuleList([
            NKATTransformerLayer(config) for _ in range(config.num_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.output_projection = nn.Sequential(
            nn.Linear(config.d_model, config.d_model // 2),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_model // 2, config.particle_types),
            nn.Softmax(dim=-1)
        )
        
        # è³ªé‡äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
        self.mass_predictor = nn.Sequential(
            nn.Linear(config.d_model, config.d_model // 2),
            nn.GELU(),
            nn.Linear(config.d_model // 2, 1)
        )
        
        # çµåˆå®šæ•°äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
        self.coupling_predictor = nn.Sequential(
            nn.Linear(config.d_model, config.d_model // 2),
            nn.GELU(),
            nn.Linear(config.d_model // 2, 1),
            nn.Sigmoid()
        )
        
    def forward(self, mass, energy, coupling, gauge_param, mask=None):
        # åŸ‹ã‚è¾¼ã¿
        x = self.embedding(mass, energy, coupling, gauge_param)
        
        # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é †æ¬¡é©ç”¨
        attention_weights = []
        for layer in self.layers:
            x, attn_weights = layer(x, mask)
            attention_weights.append(attn_weights)
        
        # å„äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®é©ç”¨
        particle_probs = self.output_projection(x)
        mass_pred = self.mass_predictor(x)
        coupling_pred = self.coupling_predictor(x)
        
        return {
            'particle_probabilities': particle_probs,
            'mass_predictions': mass_pred,
            'coupling_predictions': coupling_pred,
            'attention_weights': attention_weights
        }

class ParticleSpectrumDataset(Dataset):
    """ç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, config, num_samples=10000):
        self.config = config
        self.num_samples = num_samples
        
        # NKATäºˆæ¸¬ç²’å­ã®åŸºæº–è³ªé‡ (GeV)
        self.base_masses = torch.tensor([
            2.08e-32,   # QIM
            2.05e-26,   # QEP
            1.65e-23,   # TPO
            1.22e14,    # NQG
            4.83e16,    # HDC
            2.42e22     # NCM
        ], dtype=torch.float32)
        
        self.generate_data()
        
    def generate_data(self):
        """ç‰©ç†çš„ã«å¦¥å½“ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        logger.info("ğŸ”¬ ç‰©ç†ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹...")
        
        # è³ªé‡ã‚¹ãƒšã‚¯ãƒˆãƒ«ç”Ÿæˆ
        mass_variations = torch.randn(self.num_samples, self.config.max_seq_len)
        mass_variations = mass_variations * 0.1  # 10%ã®å¤‰å‹•
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ¬ãƒ™ãƒ«
        energy_base = torch.logspace(-32, 22, self.config.max_seq_len)
        energy_variations = torch.randn(self.num_samples, self.config.max_seq_len) * 0.05
        
        # çµåˆå®šæ•°ï¼ˆãƒŸãƒ¥ãƒ¼ã‚ªãƒ³g-2ã‹ã‚‰å°å‡ºï¼‰
        coupling_base = self.config.fifth_force_strength
        coupling_variations = torch.randn(self.num_samples, self.config.max_seq_len) * coupling_base * 0.1
        
        # ã‚²ãƒ¼ã‚¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        gauge_params = torch.rand(self.num_samples, self.config.max_seq_len) * 2 * np.pi
        
        self.masses = mass_variations + energy_base.log10().unsqueeze(0)
        self.energies = energy_base.unsqueeze(0).repeat(self.num_samples, 1) * (1 + energy_variations)
        self.couplings = coupling_base + coupling_variations
        self.gauge_params = gauge_params
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆNKATç†è«–ã«åŸºã¥ãï¼‰
        self.targets = self._generate_targets()
        
        logger.info(f"âœ… {self.num_samples}ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆå®Œäº†")
        
    def _generate_targets(self):
        """NKATç†è«–ã«åŸºã¥ãã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ"""
        targets = {}
        
        # ç²’å­ç¢ºç‡åˆ†å¸ƒ
        particle_probs = torch.zeros(self.num_samples, self.config.max_seq_len, self.config.particle_types)
        for i in range(self.config.particle_types):
            prob = torch.exp(-torch.abs(self.energies - self.base_masses[i]) / self.base_masses[i])
            particle_probs[:, :, i] = prob
        
        # æ­£è¦åŒ–
        particle_probs = particle_probs / particle_probs.sum(dim=-1, keepdim=True)
        
        # è³ªé‡äºˆæ¸¬ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        mass_targets = self.energies.log10()
        
        # çµåˆå®šæ•°ç›®æ¨™
        coupling_targets = torch.abs(self.couplings)
        
        targets['particle_probabilities'] = particle_probs
        targets['mass_predictions'] = mass_targets.unsqueeze(-1)
        targets['coupling_predictions'] = coupling_targets.unsqueeze(-1)
        
        return targets
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return {
            'mass': self.masses[idx],
            'energy': self.energies[idx],
            'coupling': self.couplings[idx],
            'gauge_param': self.gauge_params[idx],
            'targets': {k: v[idx] for k, v in self.targets.items()}
        }

class NKATTrainer:
    """NKAT-Transformerè¨“ç·´ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.model = NKATTransformerModel(config).to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=config.learning_rate,
            weight_decay=1e-5
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=config.learning_rate * 10,
            total_steps=config.num_epochs,
            pct_start=0.1
        )
        
        # æå¤±é–¢æ•°
        self.mse_loss = nn.MSELoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
        # è¨“ç·´è¨˜éŒ²
        self.train_history = {
            'epoch': [],
            'total_loss': [],
            'particle_loss': [],
            'mass_loss': [],
            'coupling_loss': []
        }
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†
        self.checkpoint_dir = "nkat_transformer_checkpoints"
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ãƒªã‚«ãƒãƒªãƒ¼æƒ…å ±
        self.last_save_time = time.time()
        
    def save_checkpoint(self, epoch, extra_info=None):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_history': self.train_history,
            'config': self.config.__dict__,
            'timestamp': datetime.now().isoformat()
        }
        
        if extra_info:
            checkpoint.update(extra_info)
        
        checkpoint_path = os.path.join(
            self.checkpoint_dir, 
            f"checkpoint_epoch_{epoch:04d}.pt"
        )
        torch.save(checkpoint, checkpoint_path)
        
        # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯æ›´æ–°
        latest_path = os.path.join(self.checkpoint_dir, "latest_checkpoint.pt")
        if os.path.exists(latest_path):
            os.remove(latest_path)
        torch.save(checkpoint, latest_path)
        
        logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_path}")
        
    def load_checkpoint(self, checkpoint_path=None):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        if checkpoint_path is None:
            checkpoint_path = os.path.join(self.checkpoint_dir, "latest_checkpoint.pt")
        
        if not os.path.exists(checkpoint_path):
            logger.info("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            return 0
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.train_history = checkpoint['train_history']
            
            start_epoch = checkpoint['epoch'] + 1
            logger.info(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©å…ƒ: ã‚¨ãƒãƒƒã‚¯ {start_epoch} ã‹ã‚‰å†é–‹")
            return start_epoch
            
        except Exception as e:
            logger.error(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def compute_loss(self, outputs, targets):
        """çµ±åˆæå¤±è¨ˆç®—"""
        # ç²’å­åˆ†é¡æå¤±
        particle_loss = self.ce_loss(
            outputs['particle_probabilities'].view(-1, self.config.particle_types),
            targets['particle_probabilities'].argmax(dim=-1).view(-1)
        )
        
        # è³ªé‡äºˆæ¸¬æå¤±
        mass_loss = self.mse_loss(
            outputs['mass_predictions'], 
            targets['mass_predictions']
        )
        
        # çµåˆå®šæ•°æå¤±
        coupling_loss = self.mse_loss(
            outputs['coupling_predictions'], 
            targets['coupling_predictions']
        )
        
        # é‡ã¿ä»˜ãç·æå¤±
        total_loss = (0.4 * particle_loss + 
                     0.3 * mass_loss + 
                     0.3 * coupling_loss)
        
        return {
            'total_loss': total_loss,
            'particle_loss': particle_loss,
            'mass_loss': mass_loss,
            'coupling_loss': coupling_loss
        }
    
    def train_epoch(self, dataloader, epoch):
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
        self.model.train()
        epoch_losses = {'total_loss': 0, 'particle_loss': 0, 'mass_loss': 0, 'coupling_loss': 0}
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch:04d}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
            mass = batch['mass'].to(self.device)
            energy = batch['energy'].to(self.device)
            coupling = batch['coupling'].to(self.device)
            gauge_param = batch['gauge_param'].to(self.device)
            targets = {k: v.to(self.device) for k, v in batch['targets'].items()}
            
            # å‹¾é…åˆæœŸåŒ–
            self.optimizer.zero_grad()
            
            # é †ä¼æ’­
            outputs = self.model(mass, energy, coupling, gauge_param)
            
            # æå¤±è¨ˆç®—
            losses = self.compute_loss(outputs, targets)
            
            # é€†ä¼æ’­
            losses['total_loss'].backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            self.optimizer.step()
            
            # æå¤±ç´¯ç©
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            progress_bar.set_postfix({
                'Loss': f"{losses['total_loss'].item():.6f}",
                'GPU': f"{torch.cuda.memory_allocated()/1e9:.1f}GB"
            })
            
            # è‡ªå‹•ä¿å­˜ãƒã‚§ãƒƒã‚¯
            if time.time() - self.last_save_time > self.config.auto_save_interval:
                self.save_checkpoint(epoch, {'batch_idx': batch_idx})
                self.last_save_time = time.time()
        
        # ã‚¨ãƒãƒƒã‚¯å¹³å‡æå¤±
        for key in epoch_losses:
            epoch_losses[key] /= len(dataloader)
        
        return epoch_losses
    
    def train(self, dataloader, start_epoch=0):
        """ãƒ¡ã‚¤ãƒ³è¨“ç·´ãƒ«ãƒ¼ãƒ—"""
        logger.info("ğŸš€ NKAT-Transformerè¨“ç·´é–‹å§‹")
        
        try:
            for epoch in range(start_epoch, self.config.num_epochs):
                # è¨“ç·´
                epoch_losses = self.train_epoch(dataloader, epoch)
                
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°
                self.scheduler.step()
                
                # å±¥æ­´è¨˜éŒ²
                self.train_history['epoch'].append(epoch)
                for key, value in epoch_losses.items():
                    self.train_history[key].append(value)
                
                # ãƒ­ã‚°å‡ºåŠ›
                logger.info(
                    f"Epoch {epoch:04d} - "
                    f"Total: {epoch_losses['total_loss']:.6f}, "
                    f"Particle: {epoch_losses['particle_loss']:.6f}, "
                    f"Mass: {epoch_losses['mass_loss']:.6f}, "
                    f"Coupling: {epoch_losses['coupling_loss']:.6f}"
                )
                
                # å®šæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if (epoch + 1) % self.config.checkpoint_interval == 0:
                    self.save_checkpoint(epoch)
                
                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ è¨“ç·´ä¸­æ–­ - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­...")
            self.save_checkpoint(epoch, {'interrupted': True})
        
        except Exception as e:
            logger.error(f"ğŸ’¥ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            self.save_checkpoint(epoch, {'error': str(e)})
            raise
        
        finally:
            # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            self.save_checkpoint(epoch, {'training_completed': True})
            logger.info("âœ… è¨“ç·´å®Œäº†")

class NKATSpectrumPredictor:
    """NKATç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.model = NKATTransformerModel(config).to(self.device)
        self.trainer = NKATTrainer(config)
        
    def predict_spectrum(self, energy_range=None):
        """ç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬"""
        if energy_range is None:
            energy_range = torch.logspace(-32, 22, 256)
        
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for energy in energy_range:
                # å…¥åŠ›æº–å‚™
                mass = torch.log10(energy).unsqueeze(0).unsqueeze(0).to(self.device)
                energy_tensor = energy.unsqueeze(0).unsqueeze(0).to(self.device)
                coupling = torch.tensor([[self.config.fifth_force_strength]]).to(self.device)
                gauge = torch.tensor([[0.0]]).to(self.device)
                
                # äºˆæ¸¬
                output = self.model(mass, energy_tensor, coupling, gauge)
                predictions.append({
                    'energy': energy.item(),
                    'particle_probs': output['particle_probabilities'].cpu().numpy(),
                    'mass_pred': output['mass_predictions'].cpu().numpy(),
                    'coupling_pred': output['coupling_predictions'].cpu().numpy()
                })
        
        return predictions
    
    def generate_spectrum_plot(self, predictions):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯è¦–åŒ–"""
        energies = [p['energy'] for p in predictions]
        particle_names = ['QIM', 'QEP', 'TPO', 'NQG', 'HDC', 'NCM']
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
        
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))
        
        # ç²’å­ç¢ºç‡åˆ†å¸ƒ
        for i, (name, color) in enumerate(zip(particle_names, colors)):
            probs = [p['particle_probs'][0, 0, i] for p in predictions]
            ax1.semilogx(energies, probs, label=name, color=color, linewidth=2)
        
        ax1.set_xlabel('Energy (GeV)')
        ax1.set_ylabel('Particle Probability')
        ax1.set_title('NKAT-Transformer Predicted Particle Spectrum\n(NKAT-Transformeräºˆæ¸¬ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # è³ªé‡äºˆæ¸¬
        mass_preds = [p['mass_pred'][0, 0] for p in predictions]
        ax2.semilogx(energies, mass_preds, color='black', linewidth=2)
        ax2.set_xlabel('Energy (GeV)')
        ax2.set_ylabel('Predicted Mass (log10 GeV)')
        ax2.set_title('Mass Prediction vs Energy')
        ax2.grid(True, alpha=0.3)
        
        # çµåˆå®šæ•°äºˆæ¸¬
        coupling_preds = [p['coupling_pred'][0, 0] for p in predictions]
        ax3.semilogx(energies, coupling_preds, color='red', linewidth=2)
        ax3.set_xlabel('Energy (GeV)')
        ax3.set_ylabel('Predicted Coupling Constant')
        ax3.set_title('Coupling Constant Prediction (Fifth Force)')
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f'nkat_transformer_spectrum_prediction_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“Š ã‚¹ãƒšã‚¯ãƒˆãƒ«å›³ä¿å­˜å®Œäº†")
        
        return fig

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ NKAT-Transformerç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    # è¨­å®šåˆæœŸåŒ–
    config = NKATTransformerConfig()
    logger.info(f"ğŸ–¥ï¸ CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"ğŸ® GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­...")
    dataset = ParticleSpectrumDataset(config, num_samples=5000)
    dataloader = DataLoader(
        dataset, 
        batch_size=config.batch_size, 
        shuffle=True,
        num_workers=0,  # Windowsäº’æ›æ€§
        pin_memory=torch.cuda.is_available()
    )
    
    # è¨“ç·´ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    trainer = NKATTrainer(config)
    
    # ãƒªã‚«ãƒãƒªãƒ¼ãƒã‚§ãƒƒã‚¯
    start_epoch = trainer.load_checkpoint()
    
    # è¨“ç·´å®Ÿè¡Œ
    trainer.train(dataloader, start_epoch)
    
    # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    predictor = NKATSpectrumPredictor(config)
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬
    logger.info("ğŸ”® ç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬å®Ÿè¡Œä¸­...")
    predictions = predictor.predict_spectrum()
    
    # çµæœå¯è¦–åŒ–
    predictor.generate_spectrum_plot(predictions)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = {
        'timestamp': datetime.now().isoformat(),
        'config': config.__dict__,
        'training_history': trainer.train_history,
        'predictions_summary': {
            'energy_range': [predictions[0]['energy'], predictions[-1]['energy']],
            'num_predictions': len(predictions),
            'max_particle_prob': max(p['particle_probs'].max() for p in predictions)
        },
        'gpu_info': {
            'device': str(trainer.device),
            'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0
        }
    }
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f'nkat_transformer_report_{timestamp}.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info("ğŸ¯ NKAT-Transformerç´ ç²’å­ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬å®Œäº†!")
    
    return report

if __name__ == "__main__":
    try:
        report = main()
        print("\nâœ… è¨ˆç®—å®Œäº†!")
        print(f"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«: nkat_transformer_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    except Exception as e:
        logger.error(f"ğŸ’¥ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        raise 