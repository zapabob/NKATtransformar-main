import torch
import torch.nn.functional as F
from torch import nn
import math

class NKATAttention(nn.Module):
    """
    ğŸ§  NKATå¼·åŒ–ç‰ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
    LLMã‚¹ã‚¿ã‚¤ãƒ«ã®Temperatureã€Top-Kã€Top-Pï¼ˆNucleusï¼‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯¾å¿œ
    å‹•çš„NKATè£œæ­£ã§å±¤åˆ¥ç†è«–å¼·åº¦èª¿æ•´
    """
    def __init__(self, dim, heads=8, temperature=1.0,
                 top_k=None, top_p=None, nkat_strength=0.,
                 nkat_decay=1.0, layer_idx=0, dropout=0.1):
        super().__init__()
        self.heads = heads
        self.scale = (dim // heads) ** -0.5
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p
        self.nkat_strength = nkat_strength
        self.nkat_decay = nkat_decay
        self.layer_idx = layer_idx
        self.dim = dim
        self.head_dim = dim // heads

        # Qã€Kã€VæŠ•å½±å±¤
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        self.out = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
        
        # ğŸ”¥ NKATç†è«–å¼·åŒ–ï¼šå‹•çš„è£œæ­£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.nkat_alpha = nn.Parameter(torch.tensor(0.1))  # å­¦ç¿’å¯èƒ½ãªÎ±
        self.nkat_beta = nn.Parameter(torch.tensor(0.05))  # å­¦ç¿’å¯èƒ½ãªÎ²
        
        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¼·åŒ–
        self.register_buffer('pos_bias', torch.zeros(1, heads, 1, 1))

    def apply_temperature_scaling(self, attn_scores):
        """ğŸŒ¡ï¸ Temperature ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨"""
        return attn_scores / max(self.temperature, 1e-8)

    def apply_top_k_filtering(self, attn_scores):
        """ğŸ” Top-K ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if self.top_k is None or self.top_k <= 0:
            return attn_scores
            
        top_k = min(self.top_k, attn_scores.size(-1))
        val, idx = torch.topk(attn_scores, top_k, dim=-1)
        mask = torch.full_like(attn_scores, float('-inf'))
        return mask.scatter(-1, idx, val)

    def apply_nucleus_filtering(self, attn_scores):
        """ğŸ§¬ Nucleus (Top-P) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if self.top_p is None or self.top_p >= 1.0:
            return attn_scores
            
        sorted_scores, idx = torch.sort(attn_scores, descending=True, dim=-1)
        sorted_probs = F.softmax(sorted_scores, dim=-1)
        cumulative = sorted_probs.cumsum(dim=-1)
        
        # Top-Pã‚ˆã‚Šå¤§ãã„ç¢ºç‡è³ªé‡ã‚’æŒã¤ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯
        mask = cumulative > self.top_p
        if mask.size(-1) > 1:
            mask[..., 1:] = mask[..., :-1].clone()
            mask[..., 0] = False
        
        sorted_scores[mask] = float('-inf')
        return torch.full_like(attn_scores, float('-inf')).scatter(-1, idx, sorted_scores)

    def apply_nkat_enhancement(self, attn, x):
        """
        ğŸ¯ å‹•çš„NKATç†è«–è£œæ­£
        å±¤æ·±åº¦ã«å¿œã˜ã¦ç†è«–å¼·åº¦ã‚’èª¿æ•´ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³æ„å‘³æƒ…å ±ã‚’çµ±åˆ
        """
        if self.nkat_strength <= 0:
            return attn
            
        # å±¤ä¾å­˜æ¸›è¡°ä¿‚æ•°
        layer_coeff = self.nkat_strength * (self.nkat_decay ** self.layer_idx)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã®æ„å‘³æƒ…å ±æŠ½å‡ºï¼ˆå¹³å‡ + åˆ†æ•£ï¼‰
        token_mean = x.mean(dim=-1, keepdim=True)  # (B, N, 1)
        token_var = x.var(dim=-1, keepdim=True, unbiased=False)  # (B, N, 1)
        
        # ğŸ§® NKATç†è«–å¼ï¼šÎ¸(x) = Î±*tanh(Î¼) + Î²*sigmoid(ÏƒÂ²)
        theta_semantic = (self.nkat_alpha * torch.tanh(token_mean) + 
                         self.nkat_beta * torch.sigmoid(token_var))
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿èª¿æ•´
        nkat_modulation = 1.0 + layer_coeff * theta_semantic.unsqueeze(1)  # (B, 1, N, 1)
        return attn * nkat_modulation

    def forward(self, x):
        """
        ğŸš€ ãƒ¡ã‚¤ãƒ³ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        Temperature â†’ Top-K â†’ Top-P â†’ NKATè£œæ­£ã®é †ã§é©ç”¨
        """
        B, N, C = x.shape
        
        # Qã€Kã€Vè¨ˆç®—
        qkv = self.qkv(x).view(B, N, 3, self.heads, self.head_dim)
        q, k, v = qkv.unbind(dim=2)  # (B, N, H, D)
        q = q.transpose(1, 2)  # (B, H, N, D)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—
        attn_scores = (q @ k.transpose(-2, -1)) * self.scale
        
        # ä½ç½®ãƒã‚¤ã‚¢ã‚¹è¿½åŠ 
        attn_scores = attn_scores + self.pos_bias
        
        # ğŸŒ¡ï¸ Temperature ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        attn_scores = self.apply_temperature_scaling(attn_scores)
        
        # ğŸ” Top-K ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        attn_scores = self.apply_top_k_filtering(attn_scores)
        
        # ğŸ§¬ Nucleus (Top-P) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°  
        attn_scores = self.apply_nucleus_filtering(attn_scores)
        
        # Softmax ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿è¨ˆç®—
        attn = F.softmax(attn_scores, dim=-1)
        attn = self.dropout(attn)
        
        # ğŸ¯ NKAT å‹•çš„è£œæ­£é©ç”¨
        attn = self.apply_nkat_enhancement(attn, x)
        
        # å€¤ã¨ã®ç©ç®—
        out = (attn @ v).transpose(1, 2).reshape(B, N, C)
        return self.out(out)

    def get_attention_entropy(self):
        """ğŸ“Š ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆè§£æç”¨ï¼‰"""
        if hasattr(self, '_last_attn'):
            attn = self._last_attn
            entropy = -torch.sum(attn * torch.log(attn + 1e-8), dim=-1)
            return entropy.mean()
        return torch.tensor(0.)


class NKATTransformerBlock(nn.Module):
    """ğŸ—ï¸ NKATå¼·åŒ–ç‰ˆTransformerãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, dim, heads, mlp_ratio=4., temperature=1.0,
                 top_k=None, top_p=None, nkat_strength=0.,
                 nkat_decay=1.0, layer_idx=0, dropout=0.1):
        super().__init__()
        
        self.norm1 = nn.LayerNorm(dim)
        self.attn = NKATAttention(
            dim, heads, temperature, top_k, top_p,
            nkat_strength, nkat_decay, layer_idx, dropout
        )
        
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(dropout),
        )
        
        # Skip connection ã®å­¦ç¿’å¯èƒ½é‡ã¿
        self.skip_alpha = nn.Parameter(torch.tensor(1.0))
        self.skip_beta = nn.Parameter(torch.tensor(1.0))

    def forward(self, x):
        # Pre-norm + Skip connection with learnable weights
        x = x + self.skip_alpha * self.attn(self.norm1(x))
        x = x + self.skip_beta * self.mlp(self.norm2(x))
        return x 