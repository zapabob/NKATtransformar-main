# nkat_transformer/model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any
import math

from .modules.nkat_attention import NKATAttention, NKATTransformerBlock

class NKATVisionTransformer(nn.Module):
    """
    ğŸš€ NKATå¼·åŒ–ç‰ˆVision Transformer
    LLMã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œ + TPEæœ€é©åŒ–è¨­è¨ˆ
    """
    
    def __init__(self, 
                 img_size: int = 28,
                 patch_size: int = 4, 
                 num_classes: int = 10,
                 embed_dim: int = 512,
                 depth: int = 6,
                 num_heads: int = 8,
                 mlp_ratio: float = 4.0,
                 # ğŸŒ¡ï¸ LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 temperature: float = 1.0,
                 top_k: Optional[int] = None,
                 top_p: Optional[float] = None,
                 # ğŸ¯ NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 nkat_strength: float = 0.0,
                 nkat_decay: float = 1.0,
                 # ğŸ›¡ï¸ æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 dropout_embed: float = 0.1,
                 dropout_attn: float = 0.1,
                 dropout_mlp: float = 0.1,
                 dropout_classifier: float = 0.1,
                 label_smoothing: float = 0.0,
                 # ğŸ”§ è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                 use_conv_stem: bool = True,
                 use_learnable_pos: bool = True):
        super().__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size  
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim
        self.num_classes = num_classes
        self.depth = depth
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ï¼ˆåˆ†æç”¨ï¼‰
        self.hyperparams = {
            'temperature': temperature,
            'top_k': top_k,
            'top_p': top_p,
            'nkat_strength': nkat_strength,
            'nkat_decay': nkat_decay,
            'label_smoothing': label_smoothing
        }
        
        # ğŸ¨ ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        if use_conv_stem:
            # ConvStem: ã‚ˆã‚ŠåŠ¹æœçš„ãªç‰¹å¾´æŠ½å‡º
            self.patch_embedding = nn.Sequential(
                nn.Conv2d(1, embed_dim // 4, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(embed_dim // 4),
                nn.GELU(),
                nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(embed_dim // 2),
                nn.GELU(),
                nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=patch_size, stride=patch_size),
                nn.BatchNorm2d(embed_dim),
                nn.GELU()
            )
        else:
            # æ¨™æº–çš„ãªç·šå½¢æŠ•å½±
            self.patch_embedding = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)
        
        # ğŸ¯ ä½ç½®åŸ‹ã‚è¾¼ã¿
        if use_learnable_pos:
            self.pos_embedding = nn.Parameter(
                torch.randn(1, self.num_patches + 1, embed_dim) * 0.02
            )
        else:
            # å›ºå®šSin/Cosä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            self.register_buffer('pos_embedding', self._get_sincos_pos_embedding())
        
        # ğŸ·ï¸ ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
        
        # ğŸ“¦ å…¥åŠ›å‡¦ç†
        self.input_norm = nn.LayerNorm(embed_dim)
        self.input_dropout = nn.Dropout(dropout_embed)
        
        # ğŸ—ï¸ Transformerãƒ–ãƒ­ãƒƒã‚¯ç¾¤
        self.layers = nn.ModuleList([
            NKATTransformerBlock(
                dim=embed_dim,
                heads=num_heads,
                mlp_ratio=mlp_ratio,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                nkat_strength=nkat_strength,
                nkat_decay=nkat_decay,
                layer_idx=i,
                dropout=dropout_attn
            ) for i in range(depth)
        ])
        
        # ğŸ¯ åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.final_norm = nn.LayerNorm(embed_dim)
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_classifier),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout_classifier * 0.5),
            nn.Linear(embed_dim // 2, embed_dim // 4),
            nn.GELU(), 
            nn.Dropout(dropout_classifier * 0.5),
            nn.Linear(embed_dim // 4, num_classes)
        )
        
        # ğŸŒ¡ï¸ å‡ºåŠ›æ¸©åº¦åˆ¶å¾¡ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.logits_temperature = nn.Parameter(torch.tensor(temperature))
        
        # ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        self.label_smoothing = label_smoothing
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _get_sincos_pos_embedding(self) -> torch.Tensor:
        """Sin/Cosä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ"""
        pe = torch.zeros(self.num_patches + 1, self.embed_dim)
        position = torch.arange(0, self.num_patches + 1).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, self.embed_dim, 2).float() * 
                           -(math.log(10000.0) / self.embed_dim))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)
    
    def _init_weights(self, m):
        """é‡ã¿åˆæœŸåŒ–"""
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor, return_attention: bool = False) -> torch.Tensor:
        """
        ğŸš€ ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        
        Args:
            x: å…¥åŠ›ç”»åƒ (B, C, H, W)
            return_attention: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’è¿”ã™ã‹ã©ã†ã‹
        
        Returns:
            åˆ†é¡ãƒ­ã‚¸ãƒƒãƒˆï¼ˆã¾ãŸã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æƒ…å ±ä»˜ãï¼‰
        """
        B = x.shape[0]
        
        # ğŸ¨ ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        x = self.patch_embedding(x)  # (B, embed_dim, H/P, W/P)
        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)
        
        # ğŸ·ï¸ ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)
        
        # ğŸ¯ ä½ç½®åŸ‹ã‚è¾¼ã¿è¿½åŠ 
        x = x + self.pos_embedding
        x = self.input_norm(x)
        x = self.input_dropout(x)
        
        # ğŸ—ï¸ Transformerãƒ–ãƒ­ãƒƒã‚¯é©ç”¨
        attention_weights = []
        for i, layer in enumerate(self.layers):
            x = layer(x)
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®è¨˜éŒ²ï¼ˆè§£æç”¨ï¼‰
            if return_attention and hasattr(layer.attn, '_last_attn'):
                attention_weights.append(layer.attn._last_attn)
        
        # ğŸ¯ åˆ†é¡
        x = self.final_norm(x)
        cls_output = x[:, 0]  # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å–å¾—
        logits = self.classifier(cls_output)
        
        # ğŸŒ¡ï¸ å‡ºåŠ›æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        logits = logits / self.logits_temperature
        
        if return_attention:
            return logits, attention_weights
        return logits
    
    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        ğŸ“Š æå¤±è¨ˆç®—ï¼ˆãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å¯¾å¿œï¼‰
        
        Args:
            logits: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›
            targets: æ­£è§£ãƒ©ãƒ™ãƒ«
        
        Returns:
            è¨ˆç®—ã•ã‚ŒãŸæå¤±
        """
        if self.label_smoothing > 0:
            # ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨ï¼ˆæ‰‹å‹•å®Ÿè£…ã§Floatå‹ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            num_classes = logits.size(-1)
            targets = targets.long()  # ç¢ºå®Ÿã«Longå‹ã«ã‚­ãƒ£ã‚¹ãƒˆ
            
            # One-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            one_hot = torch.zeros_like(logits).scatter_(1, targets.unsqueeze(1), 1)
            
            # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
            smoothed_targets = one_hot * (1 - self.label_smoothing) + \
                             self.label_smoothing / num_classes
            
            # Log-softmax + NLLæå¤±
            log_probs = F.log_softmax(logits, dim=-1)
            loss = -(smoothed_targets * log_probs).sum(dim=-1).mean()
            return loss
        else:
            # æ¨™æº–CrossEntropyï¼ˆLongå‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¿è¨¼ï¼‰
            targets = targets.long()
            return F.cross_entropy(logits, targets)
    
    def get_nkat_parameters(self) -> Dict[str, Any]:
        """ğŸ” NKATé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—"""
        nkat_params = {}
        for name, param in self.named_parameters():
            if any(keyword in name.lower() for keyword in ['nkat', 'alpha', 'beta']):
                nkat_params[name] = param.data.clone()
        return nkat_params
    
    def get_model_info(self) -> Dict[str, Any]:
        """ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        nkat_params = sum(p.numel() for name, p in self.named_parameters() 
                         if any(keyword in name.lower() for keyword in ['nkat', 'alpha', 'beta']))
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'nkat_parameters': nkat_params,
            'nkat_ratio': nkat_params / max(total_params, 1),
            'hyperparameters': self.hyperparams.copy(),
            'architecture': {
                'embed_dim': self.embed_dim,
                'depth': self.depth,
                'num_patches': self.num_patches,
                'num_classes': self.num_classes
            }
        }


class NKATLightweight(NKATVisionTransformer):
    """ğŸª¶ è»½é‡ç‰ˆNKATï¼ˆé«˜é€Ÿå®Ÿé¨“ç”¨ï¼‰"""
    
    def __init__(self, **kwargs):
        # è»½é‡åŒ–è¨­å®šã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        lightweight_defaults = {
            'embed_dim': 256,
            'depth': 4,
            'num_heads': 4,
            'mlp_ratio': 2.0,
            'patch_size': 7,  # ã‚ˆã‚Šå¤§ããªãƒ‘ãƒƒãƒã‚µã‚¤ã‚º
        }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’é©ç”¨
        for key, default_value in lightweight_defaults.items():
            kwargs.setdefault(key, default_value)
        
        super().__init__(**kwargs)


class NKATHeavyweight(NKATVisionTransformer):
    """ğŸ‹ï¸ é«˜æ€§èƒ½ç‰ˆNKATï¼ˆæœ€çµ‚ç²¾åº¦é‡è¦–ï¼‰"""
    
    def __init__(self, **kwargs):
        # é«˜æ€§èƒ½è¨­å®šã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        heavyweight_defaults = {
            'embed_dim': 768,
            'depth': 12,
            'num_heads': 12,
            'mlp_ratio': 4.0,
            'patch_size': 2,  # ã‚ˆã‚Šç´°ã‹ãªãƒ‘ãƒƒãƒã‚µã‚¤ã‚º
        }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’é©ç”¨
        for key, default_value in heavyweight_defaults.items():
            kwargs.setdefault(key, default_value)
        
        super().__init__(**kwargs) 