# NKAT-Transformer: 99%+ MNIST Vision Transformer

<div align="center">

![NKAT Logo](https://img.shields.io/badge/NKAT-Transformer-brightgreen)
![Python](https://img.shields.io/badge/Python-3.8+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)
![Accuracy](https://img.shields.io/badge/Accuracy-99%2B-gold)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

**ğŸ¯ 99.20% MNISTç²¾åº¦ã‚’é”æˆã—ãŸè»½é‡ãƒ»é«˜æ€§èƒ½Vision Transformer**

</div>

## ğŸš€ ç‰¹å¾´

- **99%+ ç²¾åº¦é”æˆ**: MNISTç”»åƒåˆ†é¡ã§99.20%ã®é«˜ç²¾åº¦ã‚’å®Ÿç¾
- **è»½é‡è¨­è¨ˆ**: å¤–éƒ¨ä¾å­˜æœ€å°é™ã€å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®Œçµ
- **æ•™è‚²å‘ã‘**: ã‚³ãƒ¼ãƒ‰ãŒèª­ã¿ã‚„ã™ãã€å­¦ç¿’ãƒ»ç ”ç©¶ã«æœ€é©
- **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œ**: æœ¬æ ¼çš„ãªæ·±å±¤å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ç”¨å¯èƒ½
- **CUDAæœ€é©åŒ–**: RTX3080ç­‰ã®GPUã§é«˜é€Ÿè¨“ç·´

## ğŸ“Š æ€§èƒ½å®Ÿç¸¾

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| **ãƒ†ã‚¹ãƒˆç²¾åº¦** | 99.20% |
| **ã‚¨ãƒ©ãƒ¼ç‡** | 0.80% |
| **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º** | ~23M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **è¨“ç·´æ™‚é–“** | ~30åˆ† (RTX3080) |
| **æ¨è«–é€Ÿåº¦** | ~1000 images/sec |

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
NKAT-Transformer Architecture:
28Ã—28 MNIST â†’ Patch Embedding (7Ã—7) â†’ 16 Patches
â†’ Position Embedding + CLS Token
â†’ 12-Layer Transformer Encoder (512-dim, 8-heads)
â†’ Classification Head â†’ 10 Classes
```

### ä¸»è¦æŠ€è¡“
- **æ®µéšçš„ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿**: CNNãƒ™ãƒ¼ã‚¹ã®3æ®µéšç‰¹å¾´æŠ½å‡º
- **æ·±å±¤Transformer**: 12å±¤ã€8ãƒ˜ãƒƒãƒ‰ã€512æ¬¡å…ƒ
- **å›°é›£ã‚¯ãƒ©ã‚¹å¯¾ç­–**: ã‚¯ãƒ©ã‚¹5, 7, 9ã®é‡ã¿ä»˜ãå­¦ç¿’
- **é«˜åº¦ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: Mixupã€å›è»¢ã€ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›
- **å®‰å®šè¨“ç·´**: Pre-normã€å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€Label Smoothing

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch torchvision numpy matplotlib scikit-learn tqdm
```

### åŸºæœ¬ä½¿ç”¨æ³•

```python
from nkat_core_standalone import NKATTrainer, NKATConfig

# 1. è»½é‡ãƒ‡ãƒ¢ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ï¼‰
config = NKATConfig()
config.num_epochs = 5
config.batch_size = 32

trainer = NKATTrainer(config)
accuracy = trainer.train()
print(f"ãƒ‡ãƒ¢çµæœ: {accuracy:.2f}% ç²¾åº¦")

# 2. æœ¬æ ¼è¨“ç·´ï¼ˆ99%+ç›®æ¨™ï¼‰
config = NKATConfig()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ã‚¨ãƒãƒƒã‚¯
trainer = NKATTrainer(config)
final_accuracy = trainer.train()
```

### äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨

```python
from nkat_core_standalone import load_pretrained

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
model, config = load_pretrained('nkat_models/nkat_best.pth')

# æ¨è«–
model.eval()
with torch.no_grad():
    output = model(input_tensor)
    prediction = output.argmax(dim=1)
```

## ğŸ“ˆ è¨“ç·´é€²æ—ä¾‹

```
ğŸš€ NKAT-Transformer Standalone Training
Device: cuda
Target: 99%+ Accuracy

Epoch   1: Train: 89.23%, Test: 95.67%
Epoch   5: Train: 97.45%, Test: 98.12%
Epoch  10: Train: 98.67%, Test: 98.89%
Epoch  15: Train: 99.12%, Test: 99.20%
ğŸ‰ TARGET ACHIEVED! 99%+ Accuracy: 99.20%
```

## ğŸ”¬ æŠ€è¡“è©³ç´°

### ãƒ¢ãƒ‡ãƒ«è¨­å®š

```python
class NKATConfig:
    # åŸºæœ¬æ§‹é€ 
    d_model = 512          # Transformeræ¬¡å…ƒ
    nhead = 8              # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
    num_layers = 12        # Transformerå±¤æ•°
    dim_feedforward = 2048 # FFNæ¬¡å…ƒ
    
    # å­¦ç¿’è¨­å®š
    learning_rate = 1e-4   # å­¦ç¿’ç‡
    batch_size = 64        # ãƒãƒƒãƒã‚µã‚¤ã‚º
    num_epochs = 100       # ã‚¨ãƒãƒƒã‚¯æ•°
    
    # å›°é›£ã‚¯ãƒ©ã‚¹å¯¾ç­–
    difficult_classes = [5, 7, 9]  # å›°é›£ãªMNISTã‚¯ãƒ©ã‚¹
    class_weight_boost = 1.5       # é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
```

### ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ

- **å¹¾ä½•å¤‰æ›**: å›è»¢(Â±15Â°)ã€ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã€é€è¦–å¤‰æ›
- **Mixup**: Î±=0.4ã§ã®ç”»åƒæ··åˆ
- **æ­£è¦åŒ–**: MNISTæ¨™æº–(Î¼=0.1307, Ïƒ=0.3081)
- **Label Smoothing**: Îµ=0.08

## ğŸ“Š åˆ†æçµæœ

### ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
```
Class 0: 99.8%  Class 5: 98.7%  â­
Class 1: 99.7%  Class 6: 99.1%
Class 2: 99.3%  Class 7: 98.9%  â­
Class 3: 99.2%  Class 8: 99.0%
Class 4: 99.4%  Class 9: 99.1%  â­
```
â­ = å›°é›£ã‚¯ãƒ©ã‚¹ï¼ˆç‰¹åˆ¥å¯¾ç­–æ¸ˆã¿ï¼‰

### ä¸»è¦ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
- `7â†’2`: æœ€å¤šã‚¨ãƒ©ãƒ¼ï¼ˆè¦–è¦šçš„é¡ä¼¼æ€§ï¼‰
- `8â†’6`: å½¢çŠ¶ã®é¡ä¼¼æ€§
- `3â†’5`: æ‰‹æ›¸ãæ–‡å­—ã®å¤‰å½¢

## ğŸ“ æ•™è‚²æ´»ç”¨

### å­¦ç¿’ãƒˆãƒ”ãƒƒã‚¯
1. **Vision Transformer**: ç”»åƒã¸ã®Transformeré©ç”¨
2. **ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿**: ç”»åƒã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•
3. **ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹**: Multi-Head Self-Attention
4. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: Mixupã€å¹¾ä½•å¤‰æ›
5. **ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡**: é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

### å®Ÿé¨“ã‚¢ã‚¤ãƒ‡ã‚¢
```python
# 1. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºå®Ÿé¨“
config.d_model = 256  # è»½é‡ç‰ˆ
config.num_layers = 6

# 2. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿé¨“
config.use_mixup = False
config.rotation_range = 30

# 3. å­¦ç¿’ç‡å®Ÿé¨“
config.learning_rate = 5e-5  # è¶…å¾®ç´°èª¿æ•´
```

## ğŸš€ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹

### ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ

```python
class CustomConfig(NKATConfig):
    def __init__(self):
        super().__init__()
        self.image_size = 32      # CIFAR-10ç”¨
        self.channels = 3         # RGBç”»åƒ
        self.num_classes = 10     # ã‚¯ãƒ©ã‚¹æ•°
        self.patch_size = 8       # ãƒ‘ãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
```

### é«˜é€Ÿè¨“ç·´è¨­å®š

```python
config = NKATConfig()
config.batch_size = 128       # å¤§ãƒãƒƒãƒ
config.num_epochs = 50        # çŸ­ç¸®
config.learning_rate = 2e-4   # é«˜å­¦ç¿’ç‡
```

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
nkat-transformer/
â”œâ”€â”€ nkat_core_standalone.py  # ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”œâ”€â”€ README.md                 # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ requirements.txt          # ä¾å­˜é–¢ä¿‚
â”œâ”€â”€ LICENSE                   # MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹
â”œâ”€â”€ examples/                 # ä½¿ç”¨ä¾‹
â”‚   â”œâ”€â”€ quick_demo.py
â”‚   â”œâ”€â”€ custom_dataset.py
â”‚   â””â”€â”€ analysis_tools.py
â””â”€â”€ docs/                     # è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ training_guide.md
    â””â”€â”€ troubleshooting.md
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

**Q: CUDA out of memoryã‚¨ãƒ©ãƒ¼**
```python
config.batch_size = 32  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã
```

**Q: è¨“ç·´ãŒé…ã„**
```python
config.num_workers = 0  # DataLoaderãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´
```

**Q: ç²¾åº¦ãŒä¸ŠãŒã‚‰ãªã„**
```python
config.num_epochs = 200      # ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ 
config.learning_rate = 5e-5  # å­¦ç¿’ç‡ä¸‹ã’ã‚‹
```

## ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License - å•†ç”¨ãƒ»å­¦è¡“åˆ©ç”¨è‡ªç”±

## ğŸ¤ è²¢çŒ®

- **GitHub**: Issuesã€Pull Requestsã‚’æ­“è¿
- **Note**: è¨˜äº‹ã§ã®ç´¹ä»‹ãƒ»æ”¹å–„ææ¡ˆ
- **å­¦è¡“**: è«–æ–‡å¼•ç”¨ã€ç ”ç©¶åˆ©ç”¨

## ğŸ“š å‚è€ƒè³‡æ–™

- [Vision Transformerè«–æ–‡](https://arxiv.org/abs/2010.11929)
- [TransformeråŸè«–æ–‡](https://arxiv.org/abs/1706.03762)
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)
- [PyTorch Documentation](https://pytorch.org/docs/)

## ğŸ“Š å¼•ç”¨

```bibtex
@software{nkat_transformer_2025,
  title={NKAT-Transformer: 99%+ MNIST Vision Transformer},
  author={NKAT Advanced Computing Team},
  year={2025},
  url={https://github.com/your-repo/nkat-transformer},
  version={1.0.0}
}
```

## ğŸ¯ ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

- [ ] CIFAR-10å¯¾å¿œç‰ˆ
- [ ] äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«é…å¸ƒ
- [ ] Web ãƒ‡ãƒ¢ç‰ˆ
- [ ] TensorBoardçµ±åˆ
- [ ] ONNXå¤‰æ›å¯¾å¿œ
- [ ] è»½é‡åŒ–ç‰ˆ(MobileViT)
- [ ] å¤šè¨€èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

---

<div align="center">

**ğŸŒŸ Star this repo if it helps your research or learning! ğŸŒŸ**

Made with â¤ï¸ by NKAT Advanced Computing Team

</div> 