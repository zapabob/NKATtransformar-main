#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced NKAT-Transformer v2.0 - 99%+ Accuracy Target
æ”¹å–„ç‰ˆNKAT Vision Transformer - 97.79% â†’ 99%+ ç²¾åº¦å‘ä¸Š

ä¸»è¦æ”¹å–„ç‚¹:
1. æ·±å±¤åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ§‹é€ ï¼ˆ512æ¬¡å…ƒã€12å±¤ï¼‰
2. æ··åˆç²¾åº¦å•é¡Œã®è§£æ±º
3. å›°é›£ã‚¯ãƒ©ã‚¹(5,7,9)å°‚ç”¨å¯¾ç­–
4. å¼·åŒ–ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
5. å®‰å®šã—ãŸè¨“ç·´æˆ¦ç•¥

Author: NKAT Advanced Computing Team
Date: 2025-06-01
GPU: RTX3080 CUDA Optimized
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from datetime import datetime
from tqdm import tqdm
import logging
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# è‹±èªè¡¨è¨˜è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/nkat_enhanced_v2_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedNKATConfig:
    """Enhanced NKAT-Transformer Configuration"""
    
    def __init__(self):
        # åŸºæœ¬è¨­å®š
        self.image_size = 28
        self.patch_size = 7
        self.num_patches = (self.image_size // self.patch_size) ** 2  # 16
        self.channels = 1
        self.num_classes = 10
        
        # æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.d_model = 512      # 384 â†’ 512 (å¼·åŒ–)
        self.nhead = 8          # 6 â†’ 8 (å¼·åŒ–)
        self.num_layers = 12    # 8 â†’ 12 (æ·±å±¤åŒ–)
        self.dim_feedforward = 2048  # 1536 â†’ 2048 (å¼·åŒ–)
        self.dropout = 0.08     # 0.1 â†’ 0.08 (æœ€é©åŒ–)
        
        # å®‰å®šæ€§è¨­å®š
        self.use_mixed_precision = False  # å®‰å®šæ€§ã®ãŸã‚ç„¡åŠ¹åŒ–
        self.use_gradient_clipping = True
        self.max_grad_norm = 1.0
        
        # å›°é›£ã‚¯ãƒ©ã‚¹å¯¾ç­–
        self.use_class_weights = True
        self.difficult_classes = [5, 7, 9]
        self.class_weight_boost = 1.5
        
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        self.use_advanced_augmentation = True
        self.rotation_range = 15
        self.zoom_range = 0.15
        self.use_elastic_deformation = True
        self.use_random_erasing = True
        
        # è¨“ç·´æˆ¦ç•¥
        self.num_epochs = 100
        self.batch_size = 64    # 128 â†’ 64 (å®‰å®šæ€§)
        self.learning_rate = 1e-4  # 3e-4 â†’ 1e-4 (å®‰å®šæ€§)
        self.use_cosine_restart = True
        self.restart_period = 20
        
        # æ­£å‰‡åŒ–
        self.use_label_smoothing = True
        self.label_smoothing = 0.08
        self.use_mixup = True
        self.mixup_alpha = 0.4
        self.weight_decay = 2e-4
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        self.save_multiple_models = True
        self.model_variants = 3

class AdvancedDataAugmentation:
    """å¼·åŒ–ç‰ˆãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config):
        self.config = config
        
    def create_train_transforms(self):
        """è¨“ç·´ç”¨å¤‰æ›ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        transforms_list = [
            # MNISTã¯ã™ã§ã«PILImageãªã®ã§ToPILImage()ã¯ä¸è¦
            
            # å¼·åŒ–ã•ã‚ŒãŸå›è»¢
            transforms.RandomRotation(
                degrees=self.config.rotation_range,
                fill=0,
                interpolation=transforms.InterpolationMode.BILINEAR
            ),
            
            # ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ï¼ˆå¾®èª¿æ•´ï¼‰
            transforms.RandomAffine(
                degrees=0,
                translate=(0.1, 0.1),
                scale=(0.9, 1.1),
                shear=5,
                fill=0
            ),
            
            # é€è¦–å¤‰æ›ï¼ˆè»½å¾®ï¼‰
            transforms.RandomPerspective(
                distortion_scale=0.1,
                p=0.3,
                fill=0
            ),
            
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ]
        
        # ãƒ©ãƒ³ãƒ€ãƒ æ¶ˆå»
        if self.config.use_random_erasing:
            transforms_list.append(
                transforms.RandomErasing(
                    p=0.1,
                    scale=(0.02, 0.1),
                    ratio=(0.3, 3.3),
                    value=0
                )
            )
        
        return transforms.Compose(transforms_list)
    
    def create_test_transforms(self):
        """ãƒ†ã‚¹ãƒˆç”¨å¤‰æ›"""
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

class EnhancedPatchEmbedding(nn.Module):
    """æ”¹å–„ç‰ˆãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿"""
    
    def __init__(self, config):
        super().__init__()
        self.patch_size = config.patch_size
        self.d_model = config.d_model
        
        # æ®µéšçš„ç‰¹å¾´æŠ½å‡º
        self.conv_layers = nn.Sequential(
            # ç¬¬1æ®µéšï¼šä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´
            nn.Conv2d(config.channels, config.d_model // 4, 3, padding=1),
            nn.BatchNorm2d(config.d_model // 4),
            nn.GELU(),
            
            # ç¬¬2æ®µéšï¼šä¸­ãƒ¬ãƒ™ãƒ«ç‰¹å¾´
            nn.Conv2d(config.d_model // 4, config.d_model // 2, 3, padding=1),
            nn.BatchNorm2d(config.d_model // 2),
            nn.GELU(),
            
            # ç¬¬3æ®µéšï¼šé«˜ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ + ãƒ‘ãƒƒãƒåŒ–
            nn.Conv2d(config.d_model // 2, config.d_model, 
                     config.patch_size, stride=config.patch_size)
        )
        
    def forward(self, x):
        # x: (B, C, H, W) -> (B, d_model, H//patch_size, W//patch_size)
        x = self.conv_layers(x)
        # Flatten spatial dimensions: (B, d_model, num_patches)
        B, C, H, W = x.shape
        x = x.reshape(B, C, H * W).transpose(1, 2)  # (B, num_patches, d_model)
        return x

class EnhancedNKATVisionTransformer(nn.Module):
    """Enhanced NKAT Vision Transformer v2.0"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # æ”¹å–„ç‰ˆãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        self.patch_embedding = EnhancedPatchEmbedding(config)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.pos_embedding = nn.Parameter(
            torch.randn(1, config.num_patches + 1, config.d_model) * 0.02
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.cls_token = nn.Parameter(
            torch.randn(1, 1, config.d_model) * 0.02
        )
        
        # å…¥åŠ›æ­£è¦åŒ–
        self.input_norm = nn.LayerNorm(config.d_model)
        
        # Enhanced Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.d_model,
            nhead=config.nhead,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-norm for stability
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, config.num_layers)
        
        # æ·±å±¤åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(config.d_model),
            nn.Dropout(config.dropout * 0.5),
            
            # ç¬¬1å±¤
            nn.Linear(config.d_model, config.d_model // 2),
            nn.GELU(),
            nn.Dropout(config.dropout),
            
            # ç¬¬2å±¤
            nn.Linear(config.d_model // 2, config.d_model // 4),
            nn.GELU(),
            nn.Dropout(config.dropout * 0.5),
            
            # å‡ºåŠ›å±¤
            nn.Linear(config.d_model // 4, config.num_classes)
        )
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, m):
        """é‡ã¿åˆæœŸåŒ–"""
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        B = x.shape[0]
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        x = self.patch_embedding(x)  # (B, num_patches, d_model)
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches + 1, d_model)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿è¿½åŠ 
        x = x + self.pos_embedding
        x = self.input_norm(x)
        
        # Transformerå‡¦ç†
        x = self.transformer(x)
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºã—ã¦åˆ†é¡
        cls_output = x[:, 0]  # (B, d_model)
        logits = self.classifier(cls_output)
        
        return logits

def create_class_weighted_sampler(dataset, config):
    """å›°é›£ã‚¯ãƒ©ã‚¹å¯¾ç­–ç”¨é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ©ãƒ¼"""
    # ç°¡å˜ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šå¤‰æ›ãªã—ã§ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
    targets = torch.tensor([dataset.targets[i] for i in range(len(dataset))])
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆ
    class_counts = torch.zeros(config.num_classes)
    for label in targets:
        class_counts[label] += 1
    
    # åŸºæœ¬é‡ã¿ã®è¨ˆç®—ï¼ˆé€†é »åº¦ï¼‰
    class_weights = 1.0 / class_counts
    
    # å›°é›£ã‚¯ãƒ©ã‚¹ã®é‡ã¿ã‚’å¼·åŒ–
    for difficult_class in config.difficult_classes:
        class_weights[difficult_class] *= config.class_weight_boost
    
    # ã‚µãƒ³ãƒ—ãƒ«é‡ã¿
    sample_weights = [class_weights[label] for label in targets]
    
    return WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )

def mixup_data(x, y, alpha=1.0):
    """Mixup ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixupç”¨æå¤±é–¢æ•°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

class EnhancedTrainer:
    """Enhanced Training Pipeline"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        self.model = EnhancedNKATVisionTransformer(config).to(self.device)
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"Enhanced model parameters: {total_params:,}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        self.train_loader, self.test_loader = self._create_dataloaders()
        
        # æœ€é©åŒ–è¨­å®š
        self.optimizer, self.scheduler = self._create_optimizer()
        self.criterion = self._create_criterion()
        
        # è¨“ç·´è¨˜éŒ²
        self.train_history = {
            'loss': [], 'accuracy': [], 'lr': []
        }
        self.test_history = {
            'loss': [], 'accuracy': []
        }
        
    def _create_dataloaders(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
        augmentation = AdvancedDataAugmentation(self.config)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        train_transform = augmentation.create_train_transforms()
        train_dataset = torchvision.datasets.MNIST(
            root="data", train=True, download=True, transform=train_transform
        )
        
        if self.config.use_class_weights:
            sampler = create_class_weighted_sampler(train_dataset, self.config)
            shuffle = False
        else:
            sampler = None
            shuffle = True
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            sampler=sampler,
            shuffle=shuffle,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_transform = augmentation.create_test_transforms()
        test_dataset = torchvision.datasets.MNIST(
            root="data", train=False, download=True, transform=test_transform
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size * 2,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        logger.info(f"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}")
        return train_loader, test_loader
    
    def _create_optimizer(self):
        """æœ€é©åŒ–å™¨ä½œæˆ"""
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        if self.config.use_cosine_restart:
            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, 
                T_0=self.config.restart_period,
                T_mult=2,
                eta_min=self.config.learning_rate * 0.01
            )
        else:
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.config.num_epochs,
                eta_min=self.config.learning_rate * 0.1
            )
        
        return optimizer, scheduler
    
    def _create_criterion(self):
        """æå¤±é–¢æ•°ä½œæˆ"""
        if self.config.use_label_smoothing:
            criterion = nn.CrossEntropyLoss(
                label_smoothing=self.config.label_smoothing
            )
        else:
            criterion = nn.CrossEntropyLoss()
        
        return criterion
    
    def train_epoch(self, epoch):
        """1ã‚¨ãƒãƒƒã‚¯è¨“ç·´"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.num_epochs}')
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            # Mixupé©ç”¨
            if self.config.use_mixup and np.random.random() < 0.5:
                data, target_a, target_b, lam = mixup_data(data, target, self.config.mixup_alpha)
                
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = mixup_criterion(self.criterion, output, target_a, target_b, lam)
            else:
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
            
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            if self.config.use_gradient_clipping:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.max_grad_norm
                )
            
            self.optimizer.step()
            
            # çµ±è¨ˆæ›´æ–°
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # é€²æ—è¡¨ç¤º
            if batch_idx % 100 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'LR': f'{current_lr:.6f}'
                })
        
        # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆ
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        current_lr = self.optimizer.param_groups[0]['lr']
        
        self.train_history['loss'].append(avg_loss)
        self.train_history['accuracy'].append(accuracy)
        self.train_history['lr'].append(current_lr)
        
        return avg_loss, accuracy
    
    def evaluate(self):
        """ãƒ†ã‚¹ãƒˆè©•ä¾¡"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in tqdm(self.test_loader, desc='Evaluating'):
                data, target = data.to(self.device), target.to(self.device)
                
                # æ··åˆç²¾åº¦ã‚’ä½¿ç”¨ã›ãšå®‰å®šè©•ä¾¡
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
                
                all_preds.extend(pred.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        avg_loss = total_loss / len(self.test_loader)
        accuracy = 100. * correct / total
        
        self.test_history['loss'].append(avg_loss)
        self.test_history['accuracy'].append(accuracy)
        
        return avg_loss, accuracy, all_preds, all_targets
    
    def train(self):
        """å®Œå…¨è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        logger.info("Starting Enhanced NKAT-Transformer Training")
        logger.info("Target: 99%+ Test Accuracy")
        
        best_accuracy = 0
        patience_counter = 0
        patience = 15
        
        for epoch in range(self.config.num_epochs):
            # è¨“ç·´
            train_loss, train_acc = self.train_epoch(epoch)
            
            # è©•ä¾¡
            test_loss, test_acc, preds, targets = self.evaluate()
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            self.scheduler.step()
            
            # ãƒ­ã‚°å‡ºåŠ›
            logger.info(f'Epoch {epoch+1:3d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                       f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if test_acc > best_accuracy:
                best_accuracy = test_acc
                patience_counter = 0
                
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'test_accuracy': test_acc,
                    'config': self.config.__dict__
                }
                
                torch.save(checkpoint, 'checkpoints/nkat_enhanced_v2_best.pth')
                logger.info(f'New best model saved! Accuracy: {test_acc:.2f}%')
                
                # è©³ç´°åˆ†æï¼ˆãƒ™ã‚¹ãƒˆæ™‚ã®ã¿ï¼‰
                if test_acc > 98.5:  # 98.5%ã‚’è¶…ãˆãŸå ´åˆã®ã¿è©³ç´°åˆ†æ
                    self.detailed_analysis(preds, targets, epoch)
            else:
                patience_counter += 1
            
            # Early stopping
            if patience_counter >= patience:
                logger.info(f'Early stopping at epoch {epoch+1}')
                break
            
            # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
            if test_acc >= 99.0:
                logger.info(f'ğŸ‰ TARGET ACHIEVED! 99%+ Accuracy: {test_acc:.2f}%')
                break
        
        logger.info(f'Training completed! Best accuracy: {best_accuracy:.2f}%')
        return best_accuracy
    
    def detailed_analysis(self, preds, targets, epoch):
        """è©³ç´°åˆ†æã¨å¯è¦–åŒ–"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(targets, preds)
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦è¨ˆç®—
        class_accuracies = {}
        for i in range(10):
            class_mask = np.array(targets) == i
            if np.sum(class_mask) > 0:
                class_acc = np.sum(np.array(preds)[class_mask] == i) / np.sum(class_mask) * 100
                class_accuracies[i] = class_acc
        
        # å¯è¦–åŒ–
        plt.figure(figsize=(15, 10))
        
        # æ··åŒè¡Œåˆ—
        plt.subplot(2, 2, 1)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=range(10), yticklabels=range(10))
        plt.title(f'Confusion Matrix - Epoch {epoch+1}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
        plt.subplot(2, 2, 2)
        classes = list(class_accuracies.keys())
        accuracies = list(class_accuracies.values())
        colors = ['red' if cls in self.config.difficult_classes else 'blue' for cls in classes]
        
        bars = plt.bar(classes, accuracies, color=colors, alpha=0.7)
        plt.title('Class-wise Accuracy')
        plt.xlabel('Class')
        plt.ylabel('Accuracy (%)')
        plt.ylim(95, 100)
        
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        # è¨“ç·´å±¥æ­´
        plt.subplot(2, 2, 3)
        plt.plot(self.train_history['accuracy'], label='Train', color='blue')
        plt.plot(self.test_history['accuracy'], label='Test', color='red')
        plt.title('Training Progress')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­¦ç¿’ç‡å±¥æ­´
        plt.subplot(2, 2, 4)
        plt.plot(self.train_history['lr'], color='green')
        plt.title('Learning Rate Schedule')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'figures/nkat_enhanced_v2_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report = {
            'timestamp': timestamp,
            'epoch': epoch + 1,
            'test_accuracy': float(np.mean(np.array(preds) == np.array(targets)) * 100),
            'class_accuracies': {str(k): float(v) for k, v in class_accuracies.items()},
            'confusion_matrix': cm.tolist(),
            'difficult_classes_performance': {
                str(cls): float(class_accuracies.get(cls, 0)) 
                for cls in self.config.difficult_classes
            }
        }
        
        with open(f'analysis/nkat_enhanced_v2_report_{timestamp}.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f'Detailed analysis saved: {timestamp}')

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Enhanced NKAT-Transformer v2.0 Training")
    print("=" * 60)
    print("Target: 97.79% â†’ 99%+ Accuracy")
    print("GPU: RTX3080 CUDA Optimized")
    print("=" * 60)
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    os.makedirs('figures', exist_ok=True)
    os.makedirs('analysis', exist_ok=True)
    
    # è¨­å®š
    config = EnhancedNKATConfig()
    
    print(f"\nğŸ“Š Enhanced Model Configuration:")
    print(f"â€¢ d_model: {config.d_model} (384â†’512)")
    print(f"â€¢ Layers: {config.num_layers} (8â†’12)")
    print(f"â€¢ Attention heads: {config.nhead} (6â†’8)")
    print(f"â€¢ Mixed precision: {config.use_mixed_precision} (Disabled for stability)")
    print(f"â€¢ Class weights: {config.use_class_weights} (Difficult classes: {config.difficult_classes})")
    print(f"â€¢ Advanced augmentation: {config.use_advanced_augmentation}")
    print(f"â€¢ Training epochs: {config.num_epochs}")
    
    # CUDAæƒ…å ±
    if torch.cuda.is_available():
        print(f"\nğŸ® GPU Information:")
        print(f"â€¢ Device: {torch.cuda.get_device_name(0)}")
        print(f"â€¢ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"â€¢ CUDA Version: {torch.version.cuda}")
    
    # è¨“ç·´é–‹å§‹
    trainer = EnhancedTrainer(config)
    best_accuracy = trainer.train()
    
    print(f"\nğŸ¯ Training Results:")
    print(f"â€¢ Best Test Accuracy: {best_accuracy:.2f}%")
    print(f"â€¢ Target Achievement: {'âœ… SUCCESS' if best_accuracy >= 99.0 else 'ğŸ”„ CONTINUE'}")
    
    if best_accuracy >= 99.0:
        print("\nğŸ‰ CONGRATULATIONS!")
        print("Enhanced NKAT-Transformer has achieved 99%+ accuracy!")
        print("Ready for production deployment.")
    else:
        print(f"\nğŸ“ˆ Progress: {best_accuracy:.2f}% / 99.0%")
        print("Consider further optimization strategies.")
    
    print("\n" + "=" * 60)
    print("Enhanced NKAT-Transformer v2.0 Training Complete!")
    print("=" * 60)

if __name__ == "__main__":
    main() 