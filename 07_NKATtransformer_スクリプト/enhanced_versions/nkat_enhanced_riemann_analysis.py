#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKATç†è«–æ”¹è‰¯ç‰ˆ - ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®é«˜ç²¾åº¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
Enhanced NKAT Theory for Riemann Hypothesis Analysis

ç‰¹å¾´ï¼š
- è‡¨ç•Œç·šä¸Šã§ã®ç²¾å¯†è§£æ
- GPUåŠ é€Ÿã«ã‚ˆã‚‹å¤§è¦æ¨¡è¨ˆç®—
- è¶…åæŸå› å­ã®å³å¯†å®Ÿè£…
- Hilbert-PÃ³lyaæŒ‡ä»¤ã®å…·ä½“åŒ–

Author: NKAT Research Team
Date: 2025-01-23
Version: 2.0 - Enhanced Analysis
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import gamma as gamma_func, zeta, digamma
from scipy.linalg import eigvals, eigvalsh
from tqdm import tqdm
import logging
import warnings
import time
warnings.filterwarnings('ignore')

# GPUåŠ é€Ÿã®è©¦è¡Œ
try:
    import cupy as cp
    GPU_AVAILABLE = True
    print("ğŸš€ GPU (CuPy) åŠ é€ŸãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    GPU_AVAILABLE = False
    print("ğŸ’» CPUè¨ˆç®—ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")

# é«˜ç²¾åº¦è¨ˆç®—
try:
    import mpmath
    mpmath.mp.dps = 50  # 50æ¡ç²¾åº¦
    HIGH_PRECISION = True
except ImportError:
    HIGH_PRECISION = False

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedNKATFramework:
    """æ”¹è‰¯ç‰ˆNKATç†è«–çš„æ çµ„ã¿"""
    
    def __init__(self, use_gpu=False):
        """åˆæœŸåŒ–"""
        logger.info("ğŸŒŸ æ”¹è‰¯ç‰ˆNKATç†è«–çš„æ çµ„ã¿åˆæœŸåŒ–é–‹å§‹")
        
        self.use_gpu = use_gpu and GPU_AVAILABLE
        self.xp = cp if self.use_gpu else np
        
        # æ•°å­¦å®šæ•°ï¼ˆé«˜ç²¾åº¦ï¼‰
        if HIGH_PRECISION:
            self.euler_gamma = float(mpmath.euler)
            self.pi = float(mpmath.pi)
            self.zeta_2 = float(mpmath.zeta(2))
        else:
            self.euler_gamma = 0.5772156649015329
            self.pi = np.pi
            self.zeta_2 = np.pi**2 / 6
        
        # NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
        self.theta = 1e-15  # éå¯æ›æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ˆã‚Šå°ã•ãï¼‰
        self.kappa = 1e-12  # KAå¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.N_c = self.pi * np.exp(1) * np.log(2)  # ç‰¹æ€§ã‚¹ã‚±ãƒ¼ãƒ«
        
        # ãƒªãƒ¼ãƒãƒ³é›¶ç‚¹ï¼ˆæœ€åˆã®10å€‹ã®è™šéƒ¨ï¼‰
        self.riemann_zeros = [
            14.134725141734693790, 21.022039638771554993, 25.010857580145688763,
            30.424876125859513210, 32.935061587739189691, 37.586178158825671257,
            40.918719012147495187, 43.327073280914999519, 48.005150881167159727,
            49.773832477672302181
        ]
        
        logger.info(f"ğŸ”¬ éå¯æ›æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸ = {self.theta:.2e}")
        logger.info(f"ğŸ”¬ KAå¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Îº = {self.kappa:.2e}")
        logger.info(f"ğŸ”¬ GPUåŠ é€Ÿ: {'æœ‰åŠ¹' if self.use_gpu else 'ç„¡åŠ¹'}")
        
    def construct_enhanced_nkat_operator(self, N: int) -> np.ndarray:
        """
        æ”¹è‰¯ç‰ˆNKATä½œç”¨ç´ ã®æ§‹ç¯‰
        
        H_N = H_0 + H_int + H_nc + H_ka
        
        Args:
            N: è¡Œåˆ—æ¬¡å…ƒ
            
        Returns:
            H_N: æ”¹è‰¯ç‰ˆNKATä½œç”¨ç´ 
        """
        logger.info(f"ğŸ”§ æ”¹è‰¯ç‰ˆNKATä½œç”¨ç´ æ§‹ç¯‰é–‹å§‹: N={N}")
        
        if self.use_gpu:
            xp = cp
        else:
            xp = np
        
        # åŸºæœ¬ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ï¼ˆWeylæ¼¸è¿‘å…¬å¼ï¼‰
        j_indices = xp.arange(N, dtype=xp.float64)
        
        # ä¸»è¦é …ï¼š(j + 1/2)Ï€/N
        main_term = (j_indices + 0.5) * self.pi / N
        
        # ã‚ªã‚¤ãƒ©ãƒ¼-ãƒã‚¹ã‚±ãƒ­ãƒ¼ãƒ‹è£œæ­£
        euler_correction = self.euler_gamma / (N * self.pi)
        
        # å¯¾æ•°è£œæ­£é …ï¼ˆã‚ˆã‚Šç²¾å¯†ï¼‰
        log_correction = (xp.log(N) / (N**2)) * xp.sin(2 * self.pi * j_indices / N)
        
        # ç´ æ•°è£œæ­£é …ï¼ˆæ•°è«–çš„æ§‹é€ ï¼‰
        prime_correction = self._compute_prime_correction(j_indices, N, xp)
        
        # ç·ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½
        energy_levels = main_term + euler_correction + log_correction + prime_correction
        
        # å¯¾è§’è¡Œåˆ—
        H_0 = xp.diag(energy_levels.astype(xp.complex128))
        
        # ç›¸äº’ä½œç”¨é …ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        H_int = self._construct_interaction_matrix(N, xp)
        
        # éå¯æ›è£œæ­£é …
        H_nc = self._construct_noncommutative_correction(N, xp)
        
        # KAå¤‰å½¢é …
        H_ka = self._construct_ka_deformation(N, xp)
        
        # ç·ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
        H_N = H_0 + H_int + H_nc + H_ka
        
        # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆæ€§ã®å³å¯†ä¿è¨¼
        H_N = 0.5 * (H_N + H_N.conj().T)
        
        # GPUâ†’CPUå¤‰æ›
        if self.use_gpu:
            H_N = cp.asnumpy(H_N)
        
        logger.info(f"âœ… æ”¹è‰¯ç‰ˆNKATä½œç”¨ç´ æ§‹ç¯‰å®Œäº†: shape={H_N.shape}")
        return H_N
    
    def _compute_prime_correction(self, j_indices, N, xp):
        """ç´ æ•°ã«åŸºã¥ãæ•°è«–çš„è£œæ­£"""
        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]
        correction = xp.zeros_like(j_indices, dtype=xp.float64)
        
        for p in primes:
            if p <= N:
                # ç´ æ•°å®šç†ã«åŸºã¥ãè£œæ­£
                prime_contrib = (xp.log(p) / p) * xp.sin(2 * self.pi * j_indices * p / N) / N**2
                correction += prime_contrib
        
        return correction
    
    def _construct_interaction_matrix(self, N, xp):
        """æ”¹è‰¯ç‰ˆç›¸äº’ä½œç”¨è¡Œåˆ—"""
        H_int = xp.zeros((N, N), dtype=xp.complex128)
        
        # çµåˆå®šæ•°ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
        c_0 = 0.01 / xp.sqrt(N)
        K_N = int(N**0.3)  # ã‚ˆã‚Šå±€æ‰€çš„ãªç›¸äº’ä½œç”¨
        
        for j in range(min(N, 500)):  # è¨ˆç®—åŠ¹ç‡ã®ãŸã‚åˆ¶é™
            for k in range(max(0, j-K_N), min(N, j+K_N+1)):
                if j != k:
                    # è·é›¢æ¸›è¡°
                    distance = abs(j - k)
                    decay = 1.0 / (distance + 1)**1.5
                    
                    # æŒ¯å‹•é …ï¼ˆæ•°è«–çš„æ§‹é€ ï¼‰
                    oscillation = xp.exp(1j * 2 * self.pi * (j + k) / self.N_c)
                    
                    # ç›¸äº’ä½œç”¨å¼·åº¦
                    V_jk = c_0 * decay * oscillation
                    H_int[j, k] = V_jk
        
        return H_int
    
    def _construct_noncommutative_correction(self, N, xp):
        """éå¯æ›è£œæ­£é …"""
        H_nc = xp.zeros((N, N), dtype=xp.complex128)
        
        # éå¯æ›æ§‹é€  [x, p] = iÎ¸ ã®é›¢æ•£ç‰ˆ
        for j in range(N-1):
            # éå¯¾è§’é …
            H_nc[j, j+1] = 1j * self.theta / N
            H_nc[j+1, j] = -1j * self.theta / N
            
            # å¯¾è§’è£œæ­£
            H_nc[j, j] += self.theta**2 / N**2
        
        return H_nc
    
    def _construct_ka_deformation(self, N, xp):
        """ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•-ã‚¢ãƒ¼ãƒãƒ«ãƒ‰å¤‰å½¢é …"""
        H_ka = xp.zeros((N, N), dtype=xp.complex128)
        
        # KAå¤‰å½¢ã®é›¢æ•£å®Ÿè£…
        for j in range(N):
            for k in range(N):
                if abs(j - k) <= 3:  # è¿‘æ¥é …ã®ã¿
                    # KAé–¢æ•°ã®è¿‘ä¼¼
                    ka_factor = self.kappa * xp.exp(-abs(j-k)/10) * xp.cos(self.pi * (j+k) / N)
                    H_ka[j, k] = ka_factor
        
        return H_ka
    
    def compute_enhanced_super_convergence_factor(self, N: int) -> complex:
        """
        æ”¹è‰¯ç‰ˆè¶…åæŸå› å­ã®è¨ˆç®—
        
        S(N) = 1 + Î³log(N/N_c)Î¨(N/N_c) + Î£ Î±_k exp(-kN/(2N_c))cos(kÏ€N/N_c) + é«˜æ¬¡é …
        """
        # ä¸»è¦å¯¾æ•°é …
        log_ratio = np.log(N / self.N_c)
        main_log_term = self.euler_gamma * log_ratio
        
        # Î¨é–¢æ•°ï¼ˆdigammaé–¢æ•°ï¼‰
        if HIGH_PRECISION:
            psi_value = float(mpmath.digamma(N / self.N_c))
        else:
            psi_value = digamma(N / self.N_c)
        
        # æŒ‡æ•°æ¸›è¡°é …ï¼ˆã‚ˆã‚Šå¤šãã®é …ï¼‰
        exponential_sum = 0.0
        alpha_coeffs = [0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001]
        
        for k, alpha_k in enumerate(alpha_coeffs, 1):
            exp_decay = np.exp(-k * N / (2 * self.N_c))
            cos_oscillation = np.cos(k * self.pi * N / self.N_c)
            exponential_sum += alpha_k * exp_decay * cos_oscillation
        
        # é«˜æ¬¡è£œæ­£é …
        higher_order = (self.theta * np.log(N)) / N**0.5
        
        S_N = 1.0 + main_log_term * psi_value + exponential_sum + higher_order
        
        return complex(S_N)
    
    def critical_line_analysis(self, t_values: list, N: int) -> dict:
        """
        è‡¨ç•Œç·šä¸Šã§ã®ç²¾å¯†è§£æ
        
        s = 1/2 + it ã§ã® NKAT è§£æ
        """
        logger.info(f"ğŸ¯ è‡¨ç•Œç·šè§£æé–‹å§‹: N={N}, t_values={len(t_values)}å€‹")
        
        H_N = self.construct_enhanced_nkat_operator(N)
        eigenvals = eigvalsh(H_N)
        
        results = []
        
        for t in tqdm(t_values, desc="è‡¨ç•Œç·šè§£æ"):
            s = complex(0.5, t)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«-ã‚¼ãƒ¼ã‚¿å¯¾å¿œ
            correspondence = self._compute_critical_line_correspondence(eigenvals, s, N)
            
            # è¶…åæŸå› å­
            S_N = self.compute_enhanced_super_convergence_factor(N)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åå·®
            deviation = self._compute_spectral_deviation(eigenvals, N)
            
            results.append({
                't': t,
                's': s,
                'correspondence_strength': correspondence['strength'],
                'spectral_deviation': deviation,
                'super_convergence_factor': S_N,
                'eigenvalue_density': len(eigenvals) / N
            })
        
        logger.info("âœ… è‡¨ç•Œç·šè§£æå®Œäº†")
        return {
            'N': N,
            'results': results,
            'average_correspondence': np.mean([r['correspondence_strength'] for r in results]),
            'average_deviation': np.mean([r['spectral_deviation'] for r in results])
        }
    
    def _compute_critical_line_correspondence(self, eigenvals, s, N):
        """è‡¨ç•Œç·šä¸Šã§ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«-ã‚¼ãƒ¼ã‚¿å¯¾å¿œ"""
        # æ­£ã®å›ºæœ‰å€¤ã®ã¿ä½¿ç”¨
        positive_eigenvals = eigenvals[eigenvals > 1e-10]
        
        if len(positive_eigenvals) == 0:
            return {'strength': 0.0, 'error': 'No positive eigenvalues'}
        
        # æ­£è¦åŒ–å®šæ•°
        c_N = self.pi / N
        
        # è§£ææ¥ç¶šã«ã‚ˆã‚‹è¨ˆç®—
        cutoff = 1.0
        large_eigenvals = positive_eigenvals[positive_eigenvals > cutoff]
        small_eigenvals = positive_eigenvals[positive_eigenvals <= cutoff]
        
        # å¤§ããªå›ºæœ‰å€¤ã®å¯„ä¸
        if len(large_eigenvals) > 0:
            large_contribution = np.sum(large_eigenvals**(-s))
        else:
            large_contribution = 0
        
        # å°ã•ãªå›ºæœ‰å€¤ã®æ­£å‰‡åŒ–ã•ã‚ŒãŸå¯„ä¸
        if len(small_eigenvals) > 0:
            regularization = np.exp(-small_eigenvals / cutoff)
            small_contribution = np.sum(small_eigenvals**(-s) * regularization)
        else:
            small_contribution = 0
        
        spectral_zeta = c_N * (large_contribution + small_contribution)
        
        # ç†è«–å€¤ã¨ã®æ¯”è¼ƒï¼ˆç°¡ç•¥åŒ–ï¼‰
        theoretical_magnitude = 1.0  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        
        if abs(spectral_zeta) > 1e-10:
            relative_error = abs(abs(spectral_zeta) - theoretical_magnitude) / theoretical_magnitude
            strength = max(0, 1 - relative_error)
        else:
            strength = 0.0
        
        return {
            'strength': strength,
            'spectral_zeta': spectral_zeta,
            'theoretical_magnitude': theoretical_magnitude
        }
    
    def _compute_spectral_deviation(self, eigenvals, N):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åå·®è¨ˆç®—"""
        j_indices = np.arange(N)
        
        # ç†è«–çš„ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½
        E_j = (j_indices + 0.5) * self.pi / N + self.euler_gamma / (N * self.pi)
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        theta_params = eigenvals - E_j
        
        # è‡¨ç•Œç·šã‹ã‚‰ã®åå·®
        deviation = np.mean([abs(np.real(theta) - 0.5) for theta in theta_params])
        
        return deviation
    
    def riemann_hypothesis_verification(self, N_values: list, t_range: tuple = (10, 50)) -> dict:
        """
        ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®æ•°å€¤çš„æ¤œè¨¼
        
        Args:
            N_values: æ¤œè¨¼ã™ã‚‹æ¬¡å…ƒã®ãƒªã‚¹ãƒˆ
            t_range: è‡¨ç•Œç·šä¸Šã®tå€¤ã®ç¯„å›²
            
        Returns:
            verification_results: æ¤œè¨¼çµæœ
        """
        logger.info("ğŸ” ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ•°å€¤çš„æ¤œè¨¼é–‹å§‹")
        
        # tå€¤ã®ç”Ÿæˆï¼ˆãƒªãƒ¼ãƒãƒ³é›¶ç‚¹å‘¨è¾ºï¼‰
        t_values = []
        for gamma in self.riemann_zeros[:5]:  # æœ€åˆã®5å€‹
            t_values.extend([gamma - 0.1, gamma, gamma + 0.1])
        
        verification_results = []
        
        for N in tqdm(N_values, desc="RHæ¤œè¨¼"):
            try:
                # è‡¨ç•Œç·šè§£æ
                critical_analysis = self.critical_line_analysis(t_values, N)
                
                # çŸ›ç›¾æ¤œè¨¼
                contradiction_analysis = self._verify_contradiction_bounds(N)
                
                # åæŸè§£æ
                convergence_analysis = self._analyze_convergence_properties(N)
                
                verification_results.append({
                    'N': N,
                    'critical_line_analysis': critical_analysis,
                    'contradiction_analysis': contradiction_analysis,
                    'convergence_analysis': convergence_analysis,
                    'overall_score': self._compute_verification_score(
                        critical_analysis, contradiction_analysis, convergence_analysis
                    )
                })
                
            except Exception as e:
                logger.warning(f"âš ï¸ N={N}ã§ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å…¨ä½“çš„ãªæ¤œè¨¼å¼·åº¦
        if verification_results:
            overall_verification_strength = np.mean([r['overall_score'] for r in verification_results])
        else:
            overall_verification_strength = 0.0
        
        logger.info(f"âœ… RHæ¤œè¨¼å®Œäº†: å…¨ä½“å¼·åº¦ = {overall_verification_strength:.4f}")
        
        return {
            'verification_results': verification_results,
            'overall_verification_strength': overall_verification_strength,
            'total_cases': len(verification_results)
        }
    
    def _verify_contradiction_bounds(self, N):
        """çŸ›ç›¾å¢ƒç•Œã®æ¤œè¨¼"""
        # ç†è«–çš„ä¸Šç•Œ
        C_explicit = 2 * np.sqrt(2 * self.pi)
        theoretical_upper_bound = C_explicit * np.log(N) / np.sqrt(N)
        
        # ä»®æƒ³çš„ä¸‹ç•Œï¼ˆRHå½ã®å ´åˆï¼‰
        delta_hypothetical = 0.001  # ã‚ˆã‚Šå°ã•ãªåå·®
        hypothetical_lower_bound = abs(delta_hypothetical) / (4 * np.log(N))
        
        # å®Ÿéš›ã®åå·®ï¼ˆNKATä½œç”¨ç´ ã‹ã‚‰ï¼‰
        H_N = self.construct_enhanced_nkat_operator(N)
        actual_deviation = self._compute_spectral_deviation(eigvalsh(H_N), N)
        
        # çŸ›ç›¾ã®æ¤œè¨¼
        contradiction_detected = (
            hypothetical_lower_bound > theoretical_upper_bound and
            actual_deviation <= theoretical_upper_bound
        )
        
        return {
            'theoretical_upper_bound': theoretical_upper_bound,
            'hypothetical_lower_bound': hypothetical_lower_bound,
            'actual_deviation': actual_deviation,
            'contradiction_detected': contradiction_detected,
            'bound_ratio': theoretical_upper_bound / hypothetical_lower_bound if hypothetical_lower_bound > 0 else float('inf')
        }
    
    def _analyze_convergence_properties(self, N):
        """åæŸç‰¹æ€§ã®è§£æ"""
        S_N = self.compute_enhanced_super_convergence_factor(N)
        
        # ç†è«–çš„æ¼¸è¿‘å€¤
        theoretical_asymptotic = 1 + self.euler_gamma * np.log(N / self.N_c)
        
        # åæŸèª¤å·®
        convergence_error = abs(S_N - theoretical_asymptotic) / abs(theoretical_asymptotic)
        
        # åæŸç‡
        convergence_rate = 1.0 / np.sqrt(N)  # ç†è«–äºˆæ¸¬
        
        return {
            'super_convergence_factor': S_N,
            'theoretical_asymptotic': theoretical_asymptotic,
            'convergence_error': convergence_error,
            'convergence_rate': convergence_rate,
            'convergence_quality': max(0, 1 - convergence_error)
        }
    
    def _compute_verification_score(self, critical_analysis, contradiction_analysis, convergence_analysis):
        """ç·åˆæ¤œè¨¼ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # å„è¦ç´ ã®é‡ã¿ä»˜ãå¹³å‡
        weights = {
            'critical_correspondence': 0.4,
            'contradiction_strength': 0.3,
            'convergence_quality': 0.3
        }
        
        critical_score = critical_analysis['average_correspondence']
        contradiction_score = 1.0 if contradiction_analysis['contradiction_detected'] else 0.0
        convergence_score = convergence_analysis['convergence_quality']
        
        overall_score = (
            weights['critical_correspondence'] * critical_score +
            weights['contradiction_strength'] * contradiction_score +
            weights['convergence_quality'] * convergence_score
        )
        
        return overall_score
    
    def visualize_enhanced_results(self, verification_results: dict):
        """æ”¹è‰¯ç‰ˆçµæœã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Enhanced NKAT Framework - Riemann Hypothesis Verification', fontsize=16, fontweight='bold')
        
        results = verification_results['verification_results']
        if not results:
            logger.warning("âš ï¸ å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        N_vals = [r['N'] for r in results]
        
        # 1. è‡¨ç•Œç·šå¯¾å¿œå¼·åº¦
        critical_strengths = [r['critical_line_analysis']['average_correspondence'] for r in results]
        axes[0, 0].semilogx(N_vals, critical_strengths, 'bo-', linewidth=2, markersize=6)
        axes[0, 0].set_xlabel('Dimension N')
        axes[0, 0].set_ylabel('Critical Line Correspondence')
        axes[0, 0].set_title('Critical Line Analysis')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1.1)
        
        # 2. ã‚¹ãƒšã‚¯ãƒˆãƒ«åå·®
        spectral_deviations = [r['critical_line_analysis']['average_deviation'] for r in results]
        axes[0, 1].loglog(N_vals, spectral_deviations, 'ro-', linewidth=2, markersize=6)
        
        # ç†è«–çš„ä¸Šç•Œ
        theoretical_bounds = [2 * np.sqrt(2 * self.pi) * np.log(N) / np.sqrt(N) for N in N_vals]
        axes[0, 1].loglog(N_vals, theoretical_bounds, 'g--', linewidth=2, label='Theoretical Bound')
        
        axes[0, 1].set_xlabel('Dimension N')
        axes[0, 1].set_ylabel('Spectral Deviation')
        axes[0, 1].set_title('Spectral Parameter Deviation')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. è¶…åæŸå› å­
        convergence_factors = [np.real(r['convergence_analysis']['super_convergence_factor']) for r in results]
        theoretical_asymptotic = [r['convergence_analysis']['theoretical_asymptotic'] for r in results]
        
        axes[0, 2].semilogx(N_vals, convergence_factors, 'mo-', label='S(N) Computed', linewidth=2)
        axes[0, 2].semilogx(N_vals, theoretical_asymptotic, 'c--', label='Theoretical', linewidth=2)
        axes[0, 2].set_xlabel('Dimension N')
        axes[0, 2].set_ylabel('Super-convergence Factor')
        axes[0, 2].set_title('Enhanced Super-convergence Analysis')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. çŸ›ç›¾æ¤œè¨¼
        contradiction_scores = [1.0 if r['contradiction_analysis']['contradiction_detected'] else 0.0 for r in results]
        axes[1, 0].semilogx(N_vals, contradiction_scores, 'go-', linewidth=2, markersize=8)
        axes[1, 0].set_xlabel('Dimension N')
        axes[1, 0].set_ylabel('Contradiction Detected')
        axes[1, 0].set_title('Proof by Contradiction')
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_ylim(-0.1, 1.1)
        
        # 5. åæŸå“è³ª
        convergence_qualities = [r['convergence_analysis']['convergence_quality'] for r in results]
        axes[1, 1].semilogx(N_vals, convergence_qualities, 'co-', linewidth=2, markersize=6)
        axes[1, 1].set_xlabel('Dimension N')
        axes[1, 1].set_ylabel('Convergence Quality')
        axes[1, 1].set_title('Convergence Analysis')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_ylim(0, 1.1)
        
        # 6. ç·åˆæ¤œè¨¼ã‚¹ã‚³ã‚¢
        overall_scores = [r['overall_score'] for r in results]
        axes[1, 2].semilogx(N_vals, overall_scores, 'ko-', linewidth=3, markersize=8)
        axes[1, 2].set_xlabel('Dimension N')
        axes[1, 2].set_ylabel('Overall Verification Score')
        axes[1, 2].set_title('Riemann Hypothesis Verification Score')
        axes[1, 2].grid(True, alpha=0.3)
        axes[1, 2].set_ylim(0, 1.1)
        
        plt.tight_layout()
        plt.savefig('enhanced_nkat_riemann_verification.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # è©³ç´°ã‚µãƒãƒªãƒ¼
        self._print_enhanced_summary(verification_results)
    
    def _print_enhanced_summary(self, verification_results):
        """æ”¹è‰¯ç‰ˆçµæœã‚µãƒãƒªãƒ¼"""
        print("\n" + "="*100)
        print("ğŸŒŸ æ”¹è‰¯ç‰ˆNKATç†è«– - ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã‚µãƒãƒªãƒ¼")
        print("="*100)
        
        results = verification_results['verification_results']
        if not results:
            print("âŒ æ¤œè¨¼çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # çµ±è¨ˆæƒ…å ±
        N_values = [r['N'] for r in results]
        overall_scores = [r['overall_score'] for r in results]
        critical_correspondences = [r['critical_line_analysis']['average_correspondence'] for r in results]
        spectral_deviations = [r['critical_line_analysis']['average_deviation'] for r in results]
        
        print(f"ğŸ“Š æ¤œè¨¼æ¬¡å…ƒç¯„å›²: {min(N_values)} â‰¤ N â‰¤ {max(N_values)}")
        print(f"ğŸ“ˆ å¹³å‡æ¤œè¨¼ã‚¹ã‚³ã‚¢: {np.mean(overall_scores):.4f} Â± {np.std(overall_scores):.4f}")
        print(f"ğŸ¯ å¹³å‡è‡¨ç•Œç·šå¯¾å¿œ: {np.mean(critical_correspondences):.4f}")
        print(f"ğŸ“‰ å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ«åå·®: {np.mean(spectral_deviations):.6f}")
        
        # æœ€é«˜æ€§èƒ½ã®æ¬¡å…ƒ
        best_idx = np.argmax(overall_scores)
        best_N = N_values[best_idx]
        best_score = overall_scores[best_idx]
        
        print(f"ğŸ† æœ€é«˜æ€§èƒ½: N={best_N}, ã‚¹ã‚³ã‚¢={best_score:.4f}")
        
        # ç†è«–çš„äºˆæ¸¬ã¨ã®æ¯”è¼ƒ
        final_N = N_values[-1]
        theoretical_deviation_bound = 2 * np.sqrt(2 * self.pi) * np.log(final_N) / np.sqrt(final_N)
        actual_deviation = spectral_deviations[-1]
        
        print(f"âš–ï¸ ç†è«–çš„åå·®ä¸Šç•Œ: {theoretical_deviation_bound:.6f}")
        print(f"âš–ï¸ å®Ÿéš›ã®åå·®: {actual_deviation:.6f}")
        print(f"âœ… ä¸Šç•Œæ¡ä»¶: {'æº€è¶³' if actual_deviation <= theoretical_deviation_bound else 'ä¸æº€è¶³'}")
        
        # å…¨ä½“çš„ãªçµè«–
        overall_strength = verification_results['overall_verification_strength']
        if overall_strength > 0.8:
            conclusion = "ğŸ‰ å¼·åŠ›ãªæ•°å€¤çš„è¨¼æ‹ "
        elif overall_strength > 0.6:
            conclusion = "âœ… æœ‰æœ›ãªæ•°å€¤çš„è¨¼æ‹ "
        elif overall_strength > 0.4:
            conclusion = "âš ï¸ éƒ¨åˆ†çš„ãªæ•°å€¤çš„è¨¼æ‹ "
        else:
            conclusion = "âŒ æ•°å€¤çš„è¨¼æ‹ ä¸ååˆ†"
        
        print(f"ğŸ” ç·åˆåˆ¤å®š: {conclusion} (å¼·åº¦: {overall_strength:.4f})")
        
        print("="*100)
        print("âœ… æ”¹è‰¯ç‰ˆNKATç†è«–æ¤œè¨¼å®Œäº†")
        print("="*100)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ æ”¹è‰¯ç‰ˆéå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•-ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ï¼ˆEnhanced NKATï¼‰")
    print("ğŸ“š ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®é«˜ç²¾åº¦æ•°ç†ç‰©ç†å­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ")
    print("="*100)
    
    # GPUä½¿ç”¨ã®ç¢ºèª
    use_gpu = GPU_AVAILABLE and input("ğŸš€ GPUåŠ é€Ÿã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower() == 'y'
    
    # æ”¹è‰¯ç‰ˆNKATæ çµ„ã¿ã®åˆæœŸåŒ–
    enhanced_nkat = EnhancedNKATFramework(use_gpu=use_gpu)
    
    # æ¤œè¨¼æ¬¡å…ƒã®è¨­å®š
    N_values = [100, 200, 500, 1000]
    
    print(f"ğŸ”¬ æ¤œè¨¼æ¬¡å…ƒ: {N_values}")
    print("â±ï¸ é«˜ç²¾åº¦è§£æã‚’é–‹å§‹ã—ã¾ã™...")
    
    start_time = time.time()
    
    # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®æ•°å€¤çš„æ¤œè¨¼
    verification_results = enhanced_nkat.riemann_hypothesis_verification(N_values)
    
    end_time = time.time()
    
    # çµæœã®å¯è¦–åŒ–
    enhanced_nkat.visualize_enhanced_results(verification_results)
    
    print(f"\nâ±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
    print("ğŸ‰ æ”¹è‰¯ç‰ˆNKATç†è«–è§£æå®Œäº†ï¼")
    print("ğŸ“Š çµæœã¯ 'enhanced_nkat_riemann_verification.png' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main() 