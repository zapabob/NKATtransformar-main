#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT Transformeræ·±å±¤å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼šéå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è¶…åæŸå› å­ã®ä¿‚æ•°æœ€é©åŒ–ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
NKAT Transformer Deep Learning Optimization System (RTX3080 Medium Power)

Author: å³¯å²¸ äº® (Ryo Minegishi)
Date: 2025å¹´5æœˆ28æ—¥
Version: 2.2 (RTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼æœ€é©åŒ–ç‰ˆ)
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from scipy.integrate import quad
from scipy.special import zeta
import pandas as pd
from tqdm import tqdm
import json
import math
import warnings
import gc
import time
import sys
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# GPUè¨­å®šï¼ˆRTX3080æœ€é©åŒ–ï¼‰
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPUå: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ VRAMå®¹é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

class PositionalEncoding(nn.Module):
    """
    Transformerã®ãŸã‚ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    """
    
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class NKATTransformerDataset(Dataset):
    """
    NKAT Transformerç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    """
    
    def __init__(self, N_values, target_values, sequence_length=16, noise_level=1e-6):
        """
        Args:
            N_values: æ¬¡å…ƒæ•°ã®é…åˆ—
            target_values: ç›®æ¨™è¶…åæŸå› å­å€¤
            sequence_length: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            noise_level: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
        """
        self.sequence_length = sequence_length
        
        # å…¥åŠ›æ¤œè¨¼
        if len(N_values) < sequence_length:
            raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º({len(N_values)})ãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·({sequence_length})ã‚ˆã‚Šå°ã•ã„ã§ã™")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
        self.N_values = torch.tensor(N_values, dtype=torch.float32)
        self.target_values = torch.tensor(target_values, dtype=torch.float32)
        
        # å¯¾æ•°å¤‰æ›ã§æ•°å€¤å®‰å®šæ€§ã‚’å‘ä¸Š
        self.log_N = torch.log(self.N_values + 1e-8)
        self.log_targets = torch.log(torch.clamp(self.target_values, min=1e-8))
        
        # æ­£è¦åŒ–
        self.log_N_mean = self.log_N.mean()
        self.log_N_std = self.log_N.std() + 1e-8
        self.log_targets_mean = self.log_targets.mean()
        self.log_targets_std = self.log_targets.std() + 1e-8
        
        self.log_N_norm = (self.log_N - self.log_N_mean) / self.log_N_std
        self.log_targets_norm = (self.log_targets - self.log_targets_mean) / self.log_targets_std
        
        # ãƒã‚¤ã‚ºè¿½åŠ 
        if noise_level > 0:
            noise = torch.normal(0, noise_level, size=self.log_targets_norm.shape)
            self.log_targets_norm += noise
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºæ¤œè¨¼
        self.dataset_size = max(0, len(self.N_values) - self.sequence_length + 1)
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†: {len(self.N_values)}ã‚µãƒ³ãƒ—ãƒ« â†’ {self.dataset_size}ã‚·ãƒ¼ã‚±ãƒ³ã‚¹")
        print(f"ğŸ“Š ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {sequence_length}")
        
        if self.dataset_size == 0:
            raise ValueError("æœ‰åŠ¹ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒä½œæˆã§ãã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¾ãŸã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    
    def __len__(self):
        return self.dataset_size
    
    def __getitem__(self, idx):
        if idx >= self.dataset_size:
            raise IndexError(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {idx} ãŒç¯„å›²å¤–ã§ã™ (æœ€å¤§: {self.dataset_size-1})")
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        end_idx = idx + self.sequence_length
        
        input_seq = self.log_N_norm[idx:end_idx].unsqueeze(-1)  # [seq_len, 1]
        target_seq = self.log_targets_norm[idx:end_idx]  # [seq_len]
        
        return input_seq, target_seq

class NKATTransformerModel(nn.Module):
    """
    NKATç”¨Transformerãƒ¢ãƒ‡ãƒ«ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
    """
    
    def __init__(self, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.1):
        super(NKATTransformerModel, self).__init__()
        
        self.d_model = d_model
        
        # å…¥åŠ›æŠ•å½±å±¤
        self.input_projection = nn.Linear(1, d_model)
        
        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoder = PositionalEncoding(d_model)
        
        # Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        self.parameter_head = nn.Sequential(
            nn.Linear(d_model, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3)  # Î³, Î´, t_c
        )
        
        # è¶…åæŸå› å­äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        self.convergence_head = nn.Sequential(
            nn.Linear(d_model + 3, 512),  # Transformerå‡ºåŠ› + ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1)
        )
        
        # åˆæœŸåŒ–
        self._init_weights()
        
        print(f"ğŸ§  NKAT Transformerãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.parameters()):,}")
        print(f"ğŸ’¾ æ¨å®šVRAMä½¿ç”¨é‡: {self._estimate_memory_usage():.1f} MB")
    
    def _init_weights(self):
        """é‡ã¿åˆæœŸåŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, x):
        """
        å‰å‘ãè¨ˆç®—
        
        Args:
            x: å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ [batch_size, seq_len, 1]
            
        Returns:
            è¶…åæŸå› å­ã®äºˆæ¸¬å€¤ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        batch_size, seq_len, _ = x.shape
        
        # å…¥åŠ›æŠ•å½±
        x = self.input_projection(x) * math.sqrt(self.d_model)  # [batch_size, seq_len, d_model]
        
        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model]
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)  # [batch_size, seq_len, d_model]
        
        # Transformer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        transformer_output = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]
        
        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ç”¨
        last_output = transformer_output[:, -1, :]  # [batch_size, d_model]
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆæ¸¬
        raw_params = self.parameter_head(last_output)  # [batch_size, 3]
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶ç´„é©ç”¨ï¼ˆå®‰å®šåŒ–ã®ãŸã‚ï¼‰
        gamma = torch.sigmoid(raw_params[:, 0]) * 0.3 + 0.15  # [0.15, 0.45]
        delta = torch.sigmoid(raw_params[:, 1]) * 0.04 + 0.02  # [0.02, 0.06]
        t_c = F.softplus(raw_params[:, 2]) + 12.0  # [12, âˆ) - ä¿®æ­£: F.softplusã‚’ä½¿ç”¨
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ã¦æ•°å€¤å®‰å®šæ€§ã‚’ç¢ºä¿
        gamma = torch.clamp(gamma, 0.1, 0.5)
        delta = torch.clamp(delta, 0.01, 0.08)
        t_c = torch.clamp(t_c, 10.0, 30.0)
        
        # è¶…åæŸå› å­è¨ˆç®—ç”¨ã®ç‰¹å¾´é‡
        params = torch.stack([gamma, delta, t_c], dim=1)  # [batch_size, 3]
        combined_features = torch.cat([last_output, params], dim=1)  # [batch_size, d_model+3]
        
        # è¶…åæŸå› å­äºˆæ¸¬
        log_S = self.convergence_head(combined_features)  # [batch_size, 1]
        log_S = torch.clamp(log_S, -5, 5)  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã‚¯ãƒªãƒƒãƒ—
        
        return log_S.squeeze(), gamma, delta, t_c

    def _estimate_memory_usage(self):
        """VRAMä½¿ç”¨é‡ã®æ¨å®š"""
        param_size = sum(p.numel() * 4 for p in self.parameters()) / 1024**2  # MB
        return param_size * 3  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ + å‹¾é… + ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼çŠ¶æ…‹

class NKATTransformerLoss(nn.Module):
    """
    Transformerç”¨ã®å®‰å®šåŒ–ã•ã‚ŒãŸæå¤±é–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    """
    
    def __init__(self, alpha=1.0, beta=0.05, gamma=0.001):
        super(NKATTransformerLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.mse = nn.MSELoss()
        self.huber = nn.SmoothL1Loss()
    
    def forward(self, log_S_pred, log_S_target, gamma_pred, delta_pred, tc_pred, model):
        """
        å®‰å®šåŒ–ã•ã‚ŒãŸæå¤±è¨ˆç®—
        """
        # ãƒ‡ãƒ¼ã‚¿é©åˆæå¤±ï¼ˆHuberæå¤±ã§å¤–ã‚Œå€¤ã«é ‘å¥ï¼‰
        data_loss = self.huber(log_S_pred, log_S_target)
        
        # ç‰©ç†åˆ¶ç´„æå¤±ï¼ˆè»½é‡åŒ–ï¼‰
        physics_loss = self._physics_constraints(gamma_pred, delta_pred, tc_pred)
        
        # æ­£å‰‡åŒ–æå¤±ï¼ˆè»½é‡åŒ–ï¼‰
        reg_loss = self._regularization_loss(model)
        
        total_loss = self.alpha * data_loss + self.beta * physics_loss + self.gamma * reg_loss
        
        # NaN ãƒã‚§ãƒƒã‚¯
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("âš ï¸ NaN/Infæ¤œå‡º - æå¤±ã‚’å°ã•ãªå€¤ã«è¨­å®š")
            total_loss = torch.tensor(1e-6, device=total_loss.device, requires_grad=True)
        
        return total_loss, data_loss, physics_loss, reg_loss
    
    def _physics_constraints(self, gamma_pred, delta_pred, tc_pred):
        """
        ç‰©ç†åˆ¶ç´„ã®è¨ˆç®—ï¼ˆè»½é‡åŒ–ç‰ˆï¼‰
        """
        constraints = []
        
        # 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²åˆ¶ç´„ï¼ˆè»½é‡åŒ–ï¼‰
        gamma_constraint = torch.mean(torch.relu(gamma_pred - 0.5) + torch.relu(0.1 - gamma_pred))
        delta_constraint = torch.mean(torch.relu(delta_pred - 0.08) + torch.relu(0.01 - delta_pred))
        tc_constraint = torch.mean(torch.relu(10.0 - tc_pred) + torch.relu(tc_pred - 30.0))
        
        constraints.extend([gamma_constraint, delta_constraint, tc_constraint])
        
        # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®‰å®šæ€§åˆ¶ç´„ï¼ˆè»½é‡åŒ–ï¼‰
        stability_loss = torch.mean((gamma_pred - 0.234) ** 2) * 0.1 + \
                        torch.mean((delta_pred - 0.035) ** 2) * 0.1 + \
                        torch.mean((tc_pred - 17.0) ** 2) * 0.001
        constraints.append(stability_loss)
        
        return sum(constraints)
    
    def _regularization_loss(self, model):
        """
        æ­£å‰‡åŒ–æå¤±ã®è¨ˆç®—ï¼ˆè»½é‡åŒ–ï¼‰
        """
        l2_reg = 0
        for param in model.parameters():
            if param.requires_grad:
                l2_reg += torch.norm(param) ** 2
        return l2_reg * 1e-7  # ã•ã‚‰ã«è»½ã„æ­£å‰‡åŒ–

class NKATTransformerOptimizer:
    """
    NKAT Transformeræœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
    """
    
    def __init__(self, learning_rate=3e-4, batch_size=16, num_epochs=300, sequence_length=32, 
                 patience=50, min_delta=1e-6):
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.sequence_length = sequence_length
        self.patience = patience
        self.min_delta = min_delta
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        self.model = NKATTransformerModel(
            d_model=256,
            nhead=8,
            num_layers=6,
            dim_feedforward=1024,
            dropout=0.1
        ).to(device)
        
        # æå¤±é–¢æ•°
        self.criterion = NKATTransformerLoss()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ï¼ˆAdamWã§å®‰å®šåŒ–ï¼‰
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-5,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆCosine Annealing + Warm Restartï¼‰
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=50,
            T_mult=2,
            eta_min=1e-7
        )
        
        # å±¥æ­´
        self.train_history = {
            'total_loss': [],
            'data_loss': [],
            'physics_loss': [],
            'reg_loss': [],
            'gamma_values': [],
            'delta_values': [],
            'tc_values': [],
            'learning_rates': []
        }
        
        print("ğŸš€ NKAT Transformeræœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰")
        print(f"ğŸ“Š æ—©æœŸåœæ­¢: patience={patience}, min_delta={min_delta}")
        print(f"ğŸ¯ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {sequence_length}")
    
    def generate_training_data(self, N_range=(10, 500), num_samples=800):
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        """
        print("ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰...")
        
        # æ¬¡å…ƒæ•°ã®ç”Ÿæˆï¼ˆã‚ˆã‚Šå¯†ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        N_values = np.logspace(np.log10(N_range[0]), np.log10(N_range[1]), num_samples)
        
        # ç†è«–çš„è¶…åæŸå› å­ã®è¨ˆç®—ï¼ˆé«˜ç²¾åº¦ç‰ˆï¼‰
        gamma_true = 0.234
        delta_true = 0.035
        t_c_true = 17.26
        
        target_values = []
        
        with tqdm(total=num_samples, desc="ç†è«–å€¤è¨ˆç®—", ncols=100) as pbar:
            for N in N_values:
                try:
                    # ç†è«–çš„è¶…åæŸå› å­ï¼ˆé«˜ç²¾åº¦è¨ˆç®—ï¼‰
                    integral = gamma_true * np.log(max(N / t_c_true, 1e-8))
                    if N > t_c_true:
                        integral += delta_true * (N - t_c_true) * 0.02  # ã‚ˆã‚Šç²¾å¯†ãªã‚¹ã‚±ãƒ¼ãƒ«
                    
                    # é«˜æ¬¡è£œæ­£é …ã‚’è¿½åŠ 
                    if N > 50:
                        integral += 0.001 * np.log(N / 50) ** 2
                    
                    S_theoretical = np.exp(np.clip(integral, -8, 8))  # ã‚ˆã‚Šåºƒã„ç¯„å›²
                    target_values.append(S_theoretical)
                    
                except:
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    target_values.append(1.0)
                
                pbar.update(1)
        
        target_values = np.array(target_values)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        try:
            dataset = NKATTransformerDataset(
                N_values, 
                target_values, 
                sequence_length=self.sequence_length,
                noise_level=5e-7  # ã‚ˆã‚Šä½ã„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
            )
        except ValueError as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œ...")
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
            self.sequence_length = min(self.sequence_length, num_samples // 4)
            dataset = NKATTransformerDataset(
                N_values, 
                target_values, 
                sequence_length=self.sequence_length,
                noise_level=5e-7
            )
        
        dataloader = DataLoader(
            dataset, 
            batch_size=self.batch_size, 
            shuffle=True,
            num_workers=2,  # RTX3080ã§ã¯ä¸¦åˆ—å‡¦ç†ã‚’æ´»ç”¨
            pin_memory=True if device.type == 'cuda' else False,
            drop_last=True,
            persistent_workers=True
        )
        
        print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {num_samples}ã‚µãƒ³ãƒ—ãƒ« â†’ {len(dataloader)}ãƒãƒƒãƒ")
        print(f"ğŸ“Š æ¨å®šè¨“ç·´æ™‚é–“: {len(dataloader) * self.num_epochs / 100:.1f}åˆ†")
        return dataloader, dataset
    
    def train(self, dataloader):
        """
        ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        """
        print("ğŸ“ ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        
        if len(dataloader) == 0:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã§ã™ã€‚è¨“ç·´ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return
        
        self.model.train()
        best_loss = float('inf')
        patience_counter = 0
        start_time = time.time()
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—
        epoch_pbar = tqdm(range(self.num_epochs), desc="ã‚¨ãƒãƒƒã‚¯é€²è¡Œ", ncols=100)
        
        try:
            for epoch in epoch_pbar:
                epoch_losses = {'total': 0, 'data': 0, 'physics': 0, 'reg': 0}
                epoch_params = {'gamma': [], 'delta': [], 'tc': []}
                valid_batches = 0
                
                # ãƒãƒƒãƒãƒ«ãƒ¼ãƒ—
                batch_pbar = tqdm(dataloader, desc=f"ã‚¨ãƒãƒƒã‚¯ {epoch+1}", leave=False, ncols=80)
                
                for batch_idx, (batch_input, batch_target) in enumerate(batch_pbar):
                    try:
                        batch_input = batch_input.to(device)
                        batch_target = batch_target.to(device)
                        
                        # å‰å‘ãè¨ˆç®—
                        log_S_pred, gamma_pred, delta_pred, tc_pred = self.model(batch_input)
                        
                        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                        log_S_target = batch_target[:, -1]
                        
                        # æå¤±è¨ˆç®—
                        total_loss, data_loss, physics_loss, reg_loss = self.criterion(
                            log_S_pred, log_S_target, gamma_pred, delta_pred, tc_pred, self.model
                        )
                        
                        # NaN/Infãƒã‚§ãƒƒã‚¯
                        if torch.isnan(total_loss) or torch.isinf(total_loss) or total_loss.item() > 1e6:
                            print(f"âš ï¸ ã‚¨ãƒãƒƒã‚¯ {epoch+1}, ãƒãƒƒãƒ {batch_idx}: ç•°å¸¸ãªæå¤±å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                            continue
                        
                        # é€†ä¼æ’­
                        self.optimizer.zero_grad()
                        total_loss.backward()
                        
                        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¼·åŒ–ï¼‰
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.3)
                        
                        self.optimizer.step()
                        
                        # æå¤±è¨˜éŒ²
                        epoch_losses['total'] += total_loss.item()
                        epoch_losses['data'] += data_loss.item()
                        epoch_losses['physics'] += physics_loss.item()
                        epoch_losses['reg'] += reg_loss.item()
                        
                        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
                        epoch_params['gamma'].extend(gamma_pred.detach().cpu().numpy())
                        epoch_params['delta'].extend(delta_pred.detach().cpu().numpy())
                        epoch_params['tc'].extend(tc_pred.detach().cpu().numpy())
                        
                        valid_batches += 1
                        
                        # ãƒãƒƒãƒé€²æ—æ›´æ–°
                        batch_pbar.set_postfix({
                            'Loss': f'{total_loss.item():.6f}',
                            'Î³': f'{gamma_pred.mean().item():.4f}',
                            'Î´': f'{delta_pred.mean().item():.4f}'
                        })
                        
                        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                        if batch_idx % 10 == 0:
                            torch.cuda.empty_cache() if device.type == 'cuda' else None
                        
                    except Exception as e:
                        print(f"âš ï¸ ãƒãƒƒãƒ {batch_idx} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                # ã‚¨ãƒãƒƒã‚¯å¹³å‡ã®è¨ˆç®—
                if valid_batches > 0:
                    for key in epoch_losses:
                        epoch_losses[key] /= valid_batches
                    
                    # å±¥æ­´è¨˜éŒ²
                    self.train_history['total_loss'].append(epoch_losses['total'])
                    self.train_history['data_loss'].append(epoch_losses['data'])
                    self.train_history['physics_loss'].append(epoch_losses['physics'])
                    self.train_history['reg_loss'].append(epoch_losses['reg'])
                    
                    if epoch_params['gamma']:
                        self.train_history['gamma_values'].append(np.mean(epoch_params['gamma']))
                        self.train_history['delta_values'].append(np.mean(epoch_params['delta']))
                        self.train_history['tc_values'].append(np.mean(epoch_params['tc']))
                    else:
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                        self.train_history['gamma_values'].append(0.234)
                        self.train_history['delta_values'].append(0.035)
                        self.train_history['tc_values'].append(17.26)
                    
                    self.train_history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])
                    
                    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°ï¼ˆCosineAnnealingWarmRestartsã®å ´åˆï¼‰
                    self.scheduler.step()
                    
                    # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯
                    if epoch_losses['total'] < best_loss - self.min_delta:
                        best_loss = epoch_losses['total']
                        patience_counter = 0
                        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
                        torch.save(self.model.state_dict(), 'best_nkat_transformer_model.pth')
                    else:
                        patience_counter += 1
                    
                    # ã‚¨ãƒãƒƒã‚¯é€²æ—æ›´æ–°
                    current_lr = self.optimizer.param_groups[0]['lr']
                    epoch_pbar.set_postfix({
                        'Loss': f'{epoch_losses["total"]:.6f}',
                        'Best': f'{best_loss:.6f}',
                        'Patience': f'{patience_counter}/{self.patience}',
                        'Î³': f'{np.mean(epoch_params["gamma"]) if epoch_params["gamma"] else 0.234:.4f}',
                        'LR': f'{current_lr:.2e}',
                        'VRAM': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB' if torch.cuda.is_available() else 'N/A'
                    })
                    
                    # æ—©æœŸåœæ­¢
                    if patience_counter >= self.patience:
                        print(f"\nğŸ›‘ æ—©æœŸåœæ­¢: {self.patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                        break
                    
                    # å®šæœŸçš„ãªé€²æ—è¡¨ç¤ºï¼ˆRTX3080ç‰ˆï¼‰
                    if (epoch + 1) % 25 == 0:
                        elapsed_time = time.time() - start_time
                        remaining_epochs = self.num_epochs - epoch - 1
                        estimated_remaining = elapsed_time / (epoch + 1) * remaining_epochs
                        
                        print(f"\nğŸ“Š ã‚¨ãƒãƒƒã‚¯ {epoch+1}/{self.num_epochs} (çµŒé: {elapsed_time:.1f}ç§’, æ®‹ã‚Šæ¨å®š: {estimated_remaining:.1f}ç§’):")
                        print(f"  ç·æå¤±: {epoch_losses['total']:.6f}")
                        print(f"  ãƒ‡ãƒ¼ã‚¿æå¤±: {epoch_losses['data']:.6f}")
                        print(f"  ç‰©ç†æå¤±: {epoch_losses['physics']:.6f}")
                        print(f"  ç¾åœ¨å­¦ç¿’ç‡: {current_lr:.2e}")
                        if torch.cuda.is_available():
                            print(f"  VRAMä½¿ç”¨é‡: {torch.cuda.memory_allocated()/1024**3:.1f}/{torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB")
                        if epoch_params['gamma']:
                            print(f"  å¹³å‡Î³: {np.mean(epoch_params['gamma']):.6f}")
                            print(f"  å¹³å‡Î´: {np.mean(epoch_params['delta']):.6f}")
                            print(f"  å¹³å‡t_c: {np.mean(epoch_params['tc']):.6f}")
                else:
                    print(f"âš ï¸ ã‚¨ãƒãƒƒã‚¯ {epoch+1}: æœ‰åŠ¹ãªãƒãƒƒãƒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
        except KeyboardInterrupt:
            print("\nğŸ›‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ è¨“ç·´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        finally:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            torch.cuda.empty_cache() if device.type == 'cuda' else None
            gc.collect()
        
        total_time = time.time() - start_time
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº† (ç·æ™‚é–“: {total_time:.1f}ç§’)")

    def evaluate_model(self, test_N_values):
        """
        ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆä¿®æ­£ç‰ˆï¼‰
        """
        print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
        
        self.model.eval()
        
        with torch.no_grad():
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            test_log_N = torch.log(torch.tensor(test_N_values, dtype=torch.float32) + 1e-8)
            
            # æ­£è¦åŒ–ï¼ˆè¨“ç·´æ™‚ã¨åŒã˜çµ±è¨ˆã‚’ä½¿ç”¨ï¼‰
            if hasattr(self, 'dataset'):
                test_log_N_norm = (test_log_N - self.dataset.log_N_mean) / self.dataset.log_N_std
            else:
                test_log_N_norm = test_log_N
            
            # å„ãƒ†ã‚¹ãƒˆç‚¹ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œ
            predictions = []
            gamma_values = []
            delta_values = []
            tc_values = []
            
            for i in range(len(test_N_values)):
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å½¢å¼ã«å¤‰æ›ï¼ˆæœ€å¾Œã®sequence_lengthå€‹ã®ç‚¹ã‚’ä½¿ç”¨ï¼‰
                if i >= self.sequence_length - 1:
                    start_idx = i - self.sequence_length + 1
                    end_idx = i + 1
                    test_input = test_log_N_norm[start_idx:end_idx].unsqueeze(0).unsqueeze(-1).to(device)  # [1, seq_len, 1]
                else:
                    # ä¸è¶³åˆ†ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    padding_size = self.sequence_length - i - 1
                    padded_input = torch.cat([
                        torch.zeros(padding_size, dtype=torch.float32),
                        test_log_N_norm[:i+1]
                    ])
                    test_input = padded_input.unsqueeze(0).unsqueeze(-1).to(device)  # [1, seq_len, 1]
                
                # äºˆæ¸¬
                log_S_pred, gamma_pred, delta_pred, tc_pred = self.model(test_input)
                
                # çµæœã‚’ä¿å­˜
                predictions.append(log_S_pred.cpu().numpy())
                gamma_values.append(gamma_pred.cpu().numpy())
                delta_values.append(delta_pred.cpu().numpy())
                tc_values.append(tc_pred.cpu().numpy())
            
            # ãƒªã‚¹ãƒˆã‚’numpyé…åˆ—ã«å¤‰æ›
            predictions = np.array(predictions).flatten()
            gamma_values = np.array(gamma_values).flatten()
            delta_values = np.array(delta_values).flatten()
            tc_values = np.array(tc_values).flatten()
            
            # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
            if hasattr(self, 'dataset'):
                predictions = predictions * self.dataset.log_targets_std.numpy() + self.dataset.log_targets_mean.numpy()
            
            predictions = np.exp(predictions)
        
        # çµ±è¨ˆè¨ˆç®—
        results = {
            'predictions': predictions,
            'gamma_mean': np.mean(gamma_values),
            'gamma_std': np.std(gamma_values),
            'delta_mean': np.mean(delta_values),
            'delta_std': np.std(delta_values),
            'tc_mean': np.mean(tc_values),
            'tc_std': np.std(tc_values),
            'gamma_values': gamma_values,
            'delta_values': delta_values,
            'tc_values': tc_values
        }
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†")
        print(f"ğŸ“Š æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  Î³ = {results['gamma_mean']:.6f} Â± {results['gamma_std']:.6f}")
        print(f"  Î´ = {results['delta_mean']:.6f} Â± {results['delta_std']:.6f}")
        print(f"  t_c = {results['tc_mean']:.6f} Â± {results['tc_std']:.6f}")
        
        return results
    
    def visualize_results(self, test_N_values, results):
        """
        çµæœã®å¯è¦–åŒ–
        """
        print("ğŸ“ˆ çµæœå¯è¦–åŒ–ä¸­...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è¨“ç·´æå¤±ã®æ¨ç§»
        if self.train_history['total_loss']:
            axes[0, 0].plot(self.train_history['total_loss'], label='ç·æå¤±', color='red', linewidth=2)
            axes[0, 0].plot(self.train_history['data_loss'], label='ãƒ‡ãƒ¼ã‚¿æå¤±', color='blue', linewidth=2)
            axes[0, 0].plot(self.train_history['physics_loss'], label='ç‰©ç†æå¤±', color='green', linewidth=2)
            axes[0, 0].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
            axes[0, 0].set_ylabel('æå¤±')
            axes[0, 0].set_title('Transformerè¨“ç·´æå¤±ã®æ¨ç§»')
            axes[0, 0].legend()
            axes[0, 0].set_yscale('log')
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ
        if self.train_history['gamma_values']:
            axes[0, 1].plot(self.train_history['gamma_values'], label='Î³', color='red', linewidth=2)
            axes[0, 1].axhline(y=0.234, color='red', linestyle='--', alpha=0.7, label='Î³ç†è«–å€¤')
            axes[0, 1].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
            axes[0, 1].set_ylabel('Î³å€¤')
            axes[0, 1].set_title('Î³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        if self.train_history['delta_values']:
            axes[0, 2].plot(self.train_history['delta_values'], label='Î´', color='blue', linewidth=2)
            axes[0, 2].axhline(y=0.035, color='blue', linestyle='--', alpha=0.7, label='Î´ç†è«–å€¤')
            axes[0, 2].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
            axes[0, 2].set_ylabel('Î´å€¤')
            axes[0, 2].set_title('Î´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
        
        # 3. è¶…åæŸå› å­ã®äºˆæ¸¬
        axes[1, 0].loglog(test_N_values, results['predictions'], 'b-', label='Transformeräºˆæ¸¬', linewidth=3)
        
        # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
        gamma_true, delta_true, t_c_true = 0.234, 0.035, 17.26
        theoretical_values = []
        for N in test_N_values:
            integral = gamma_true * np.log(max(N / t_c_true, 1e-8))
            if N > t_c_true:
                integral += delta_true * (N - t_c_true) * 0.1
            theoretical_values.append(np.exp(np.clip(integral, -10, 10)))
        
        axes[1, 0].loglog(test_N_values, theoretical_values, 'r--', label='ç†è«–å€¤', linewidth=2)
        axes[1, 0].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[1, 0].set_ylabel('è¶…åæŸå› å­ S(N)')
        axes[1, 0].set_title('Transformer: è¶…åæŸå› å­ã®äºˆæ¸¬vsç†è«–å€¤')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. å­¦ç¿’ç‡ã®æ¨ç§»
        if self.train_history['learning_rates']:
            axes[1, 1].plot(self.train_history['learning_rates'], color='purple', linewidth=2)
            axes[1, 1].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
            axes[1, 1].set_ylabel('å­¦ç¿’ç‡')
            axes[1, 1].set_title('å­¦ç¿’ç‡ã®æ¨ç§»')
            axes[1, 1].set_yscale('log')
            axes[1, 1].grid(True, alpha=0.3)
        
        # 5. t_c ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ
        if self.train_history['tc_values']:
            axes[1, 2].plot(self.train_history['tc_values'], label='t_c', color='green', linewidth=2)
            axes[1, 2].axhline(y=17.26, color='green', linestyle='--', alpha=0.7, label='t_cç†è«–å€¤')
            axes[1, 2].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
            axes[1, 2].set_ylabel('t_cå€¤')
            axes[1, 2].set_title('t_cãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('nkat_transformer_optimization_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å¯è¦–åŒ–å®Œäº†")
    
    def save_model_and_results(self, results, filename_prefix='nkat_transformer_optimization'):
        """
        ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜
        """
        print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_history': self.train_history,
            'results': results
        }, f'{filename_prefix}_model.pth')
        
        # çµæœã‚’JSONå½¢å¼ã§ä¿å­˜
        json_results = {
            'optimal_parameters': {
                'gamma_mean': float(results['gamma_mean']),
                'gamma_std': float(results['gamma_std']),
                'delta_mean': float(results['delta_mean']),
                'delta_std': float(results['delta_std']),
                'tc_mean': float(results['tc_mean']),
                'tc_std': float(results['tc_std'])
            },
            'training_config': {
                'learning_rate': self.learning_rate,
                'batch_size': self.batch_size,
                'num_epochs': self.num_epochs,
                'sequence_length': self.sequence_length
            },
            'final_losses': {
                'total_loss': self.train_history['total_loss'][-1] if self.train_history['total_loss'] else 0,
                'data_loss': self.train_history['data_loss'][-1] if self.train_history['data_loss'] else 0,
                'physics_loss': self.train_history['physics_loss'][-1] if self.train_history['physics_loss'] else 0,
                'reg_loss': self.train_history['reg_loss'][-1] if self.train_history['reg_loss'] else 0
            }
        }
        
        with open(f'{filename_prefix}_results.json', 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ä¿å­˜å®Œäº†: {filename_prefix}_model.pth, {filename_prefix}_results.json")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰"""
    print("ğŸš€ NKAT Transformeræ·±å±¤å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰")
    print("="*70)
    
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        optimizer = NKATTransformerOptimizer(
            learning_rate=3e-4,
            batch_size=16,  # RTX3080ã«é©ã—ãŸã‚µã‚¤ã‚º
            num_epochs=300,  # ã‚ˆã‚Šé•·ã„è¨“ç·´
            sequence_length=32,  # ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            patience=50,  # ã‚ˆã‚Šé•·ã„å¿è€
            min_delta=1e-6
        )
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰
        dataloader, dataset = optimizer.generate_training_data(
            N_range=(10, 500),  # ã‚ˆã‚Šåºƒã„ç¯„å›²
            num_samples=800  # ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜ï¼ˆè©•ä¾¡æ™‚ã«ä½¿ç”¨ï¼‰
        optimizer.dataset = dataset
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ç¢ºèª: {len(dataloader)}ãƒãƒƒãƒ")
        if len(dataloader) == 0:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        print("ğŸ“ RTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§è¨“ç·´é–‹å§‹...")
        optimizer.train(dataloader)
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        test_N_values = np.logspace(1, 2.7, 50)  # ã‚ˆã‚Šåºƒã„ç¯„å›²ã¨ã‚ˆã‚Šå¤šãã®ç‚¹
        results = optimizer.evaluate_model(test_N_values)
        
        # çµæœå¯è¦–åŒ–
        optimizer.visualize_results(test_N_values, results)
        
        # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜
        optimizer.save_model_and_results(results, 'nkat_transformer_rtx3080_medium')
        
        # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®å«æ„
        gamma_opt = results['gamma_mean']
        t_c_opt = results['tc_mean']
        riemann_convergence = gamma_opt * np.log(max(500 / t_c_opt, 1e-8))
        riemann_deviation = abs(riemann_convergence - 0.5)
        
        print("\n" + "="*70)
        print("ğŸ¯ NKAT Transformeræ·±å±¤å­¦ç¿’æœ€é©åŒ–çµæœï¼ˆRTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ç‰ˆï¼‰")
        print("="*70)
        print(f"ğŸ“Š æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  Î³ = {results['gamma_mean']:.6f} Â± {results['gamma_std']:.6f}")
        print(f"  Î´ = {results['delta_mean']:.6f} Â± {results['delta_std']:.6f}")
        print(f"  t_c = {results['tc_mean']:.6f} Â± {results['tc_std']:.6f}")
        print(f"\nğŸ¯ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®å«æ„:")
        print(f"  åæŸç‡: Î³Â·ln(500/t_c) = {riemann_convergence:.6f}")
        print(f"  ç†è«–å€¤ã‹ã‚‰ã®åå·®: {riemann_deviation:.6f}")
        print(f"  ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ”¯æŒåº¦: {100*(1-min(riemann_deviation/0.1, 1.0)):.1f}%")
        
        print("\nğŸ RTX3080ä¸­ãƒ‘ãƒ¯ãƒ¼ Transformeræ·±å±¤å­¦ç¿’æœ€é©åŒ–å®Œäº†")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”§ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦å†å®Ÿè¡Œã‚’æ¨å¥¨ã—ã¾ã™")
        import traceback
        traceback.print_exc()
    finally:
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print("ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†")

if __name__ == "__main__":
    main() 